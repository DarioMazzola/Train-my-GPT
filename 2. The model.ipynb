{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xji-JIdFsxbb"
   },
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMMDFgOEsxbd"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets plotly transformers accelerate torch\n",
    "!pip -q install --upgrade ipywidgets\n",
    "!pip -q install parlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpbtahCRsxbe"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets hnswlib\n",
    "!pip install -U -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 55370,
     "status": "ok",
     "timestamp": 1719568121184,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "0e0rNPLQsxbe"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23155,
     "status": "ok",
     "timestamp": 1719568144336,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "KciBG1dIsxbe",
    "outputId": "4380dd8c-4d64-457f-b778-bf862bb5e8b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, BitsAndBytesConfig, TextStreamer\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "from unsloth import FastLanguageModel\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "from trl import SFTTrainer\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "\n",
    "# Unsloth settings\n",
    "max_seq_length = 2048\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18667,
     "status": "ok",
     "timestamp": 1719568163000,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "Uqq_EUQOsxbe",
    "outputId": "6ace7e8c-f841-4c49-b368-cb6b096d782b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n",
      "/gdrive/My Drive/GPT: Gnocchi Pesto e Tartufo\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive', force_remount=True)\n",
    "%cd /gdrive/My\\ Drive/GPT:\\ Gnocchi\\ Pesto\\ e\\ Tartufo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1719568170420,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "i0ZpXzHIsxbf"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = \"Open-Orca/SlimOrca\"\n",
    "SYSTEM_TOKEN = 'system: '\n",
    "HUMAN_TOKEN = 'human: '\n",
    "GPT_TOKEN = 'gpt: '\n",
    "SAVE_DIRECTORY = './Models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1719568442310,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "mDdOHwi6sxbf",
    "outputId": "60cc396f-ab35-4206-9225-9ad39be8577b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjEwYveVsxbf"
   },
   "source": [
    "# Load dataset\n",
    "\n",
    "The Open-Orca/SlimOrca dataset only has the training split in HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4_THbtIsxbf"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(DATASET_NAME)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 187252,
     "status": "ok",
     "timestamp": 1716658661934,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "zbrFbNZMsxbf",
    "outputId": "ae4e4cc8-61a0-4ae7-aed2-771130f7a4ba"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0077936649322509766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading readme",
       "rate": null,
       "total": 2154,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625c3f20dae644dabbbdf5a738ec7b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019156455993652344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 985501644,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab8fff767a940789766f013bacddeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/986M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026966571807861328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 517982,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e4db5c7abf4052a1d804a7ab507e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/517982 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "perc = 0.01\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(DATASET_NAME)['train']\n",
    "dataset.set_format(\"torch\")\n",
    "\n",
    "# Identificare i diversi tipi di system prompt e contare la loro occorrenza\n",
    "system_prompt_counts = defaultdict(list)\n",
    "for idx, data in enumerate(dataset):\n",
    "    for conversation in data['conversations']:\n",
    "        if conversation['from'] == 'system':\n",
    "            prompt = conversation['value']\n",
    "            system_prompt_counts[prompt].append(idx)\n",
    "            break\n",
    "\n",
    "# Calcolare la dimensione ridotta per ogni tipo di system prompt\n",
    "total_reduced_size = int(len(dataset) * perc)\n",
    "num_system_prompts = len(system_prompt_counts)\n",
    "reduced_size_per_prompt = total_reduced_size // num_system_prompts\n",
    "\n",
    "# Creare un campione uniforme\n",
    "reduced_indices = []\n",
    "for prompt, indices in system_prompt_counts.items():\n",
    "    sampled_indices = random.sample(indices, min(reduced_size_per_prompt, len(indices)))\n",
    "    reduced_indices.extend(sampled_indices)\n",
    "\n",
    "# Verificare e compensare se ci sono meno elementi di quelli richiesti in qualche categoria\n",
    "if len(reduced_indices) < total_reduced_size:\n",
    "    remaining_size = total_reduced_size - len(reduced_indices)\n",
    "    all_indices = [idx for indices in system_prompt_counts.values() for idx in indices]\n",
    "    additional_indices = random.sample(list(set(all_indices) - set(reduced_indices)), remaining_size)\n",
    "    reduced_indices.extend(additional_indices)\n",
    "\n",
    "# Creare il reduced_dataset\n",
    "reduced_dataset = dataset.select(reduced_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzyIm8NT7Q6d"
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8J8mVsG7Un4"
   },
   "outputs": [],
   "source": [
    "def compare_responses(model, conversation, eos_token):\n",
    "\n",
    "    system_prompt = conversation.split(SYSTEM_TOKEN)[1].split(HUMAN_TOKEN)[0]\n",
    "    user_prompt = conversation.split(SYSTEM_TOKEN)[1].split(HUMAN_TOKEN)[1].split(GPT_TOKEN)[0]\n",
    "    expected_response = conversation.split(SYSTEM_TOKEN)[1].split(HUMAN_TOKEN)[1].split(GPT_TOKEN)[1]\n",
    "\n",
    "    prompt = prepare_prompt(prompt = user_prompt, eos_token = eos_token, system_prompt = system_prompt)\n",
    "\n",
    "    # Encode context\n",
    "    input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    input_ids = input_tokenized['input_ids']\n",
    "    attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=200,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=5,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    generated_response = tokenizer.decode(output_ids[0, input_ids.size(1):], skip_special_tokens=False)\n",
    "\n",
    "    print(f\"System prompt: {system_prompt}\\n\")\n",
    "    print(f\"User prompt: {user_prompt}\\n\")\n",
    "\n",
    "    print(f\"Expected response: {expected_response}\\n\")\n",
    "    print(f\"Generated response: {generated_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TrVxnLoKyC5"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_perplexity(model, conversation):\n",
    "\n",
    "    system_prompt = conversation.split(SYSTEM_TOKEN)[1].split(HUMAN_TOKEN)[0]\n",
    "    user_prompt = conversation.split(SYSTEM_TOKEN)[1].split(HUMAN_TOKEN)[1].split(GPT_TOKEN)[0]\n",
    "    expected_response = conversation.split(SYSTEM_TOKEN)[1].split(HUMAN_TOKEN)[1].split(GPT_TOKEN)[1]\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Encode the dialogue\n",
    "    input_encoding = tokenizer(system_prompt + user_prompt + expected_response, return_tensors='pt').to(device)\n",
    "\n",
    "    # Compute model outputs\n",
    "    outputs = model(**input_encoding)\n",
    "\n",
    "    # Encode the response separately for labels\n",
    "    labels = tokenizer(expected_response, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "    # Adjust logits to match the label sequence length\n",
    "    logits = outputs.logits[:, -labels.size(1):]\n",
    "\n",
    "    # Shift logits and labels to align predictions and true values\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    # Compute loss using cross-entropy\n",
    "    lm_loss = F.cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "    )\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = torch.exp(lm_loss).item()\n",
    "    return perplexity\n",
    "\n",
    "def dataset_perplexity(model, dataset):\n",
    "    cum_perplexity = 0\n",
    "\n",
    "    for conv in dataset:\n",
    "        conv = conv['text']\n",
    "        cum_perplexity += calculate_perplexity(model, conv)\n",
    "\n",
    "    return cum_perplexity / len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rtxusR2zOtH"
   },
   "outputs": [],
   "source": [
    "from parlai.core.metrics import BleuMetric\n",
    "\n",
    "def calculate_bleu(model, conversation, eos_token):\n",
    "\n",
    "    system_prompt = conversation.split(SYSTEM_TOKEN)[1].split(HUMAN_TOKEN)[0]\n",
    "    user_prompt = conversation.split(SYSTEM_TOKEN)[1].split(HUMAN_TOKEN)[1].split(GPT_TOKEN)[0]\n",
    "    expected_response = conversation.split(SYSTEM_TOKEN)[1].split(HUMAN_TOKEN)[1].split(GPT_TOKEN)[1]\n",
    "\n",
    "    prompt = prepare_prompt(prompt = user_prompt, eos_token = eos_token, system_prompt = system_prompt)\n",
    "\n",
    "    # Encode context\n",
    "    input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    input_ids = input_tokenized['input_ids']\n",
    "    attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=200,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=5,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    generated_response = tokenizer.decode(output_ids[0, input_ids.size(1):], skip_special_tokens=False)\n",
    "\n",
    "    bleu = BleuMetric.compute(generated_response, [expected_response])\n",
    "\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gQKiTxtsxbg"
   },
   "source": [
    "# Retrieval-based chatbot\n",
    "\n",
    "Our starting point for the model was a retrieval-based chatbot, because, let's face it, reinventing the wheel can be exhausting. Retrieval-based models offer a straightforward approach: they sift through existing responses to find the most suitable one for a given input. And what better way to imitate ChatGPT than to let ChatGPT do the talking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5426,
     "status": "ok",
     "timestamp": 1716556502601,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "Iw6MpbNxsxbg",
    "outputId": "0ccd5432-3381-45a4-e59c-2f106c4f08c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3095d9477ac54e679a80cf3160f4011e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e2ea8d57af4b9db40a330fd5dccded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc0d17974714ae2b1357d28f2e1d50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27bc8ccae234afeb919fc1ae4fcccb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d88d95b205403897cd2d3372f1c7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a480036289847a3975272ec12069605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b5ab228788450e8ddf0617d3b33132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de902e6cb8844429f8e783b848170fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575c3f58c53c4a7abcc35d8aa29e0689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea939828cdb4a73811971c77dd7ec75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3446e6dfa264b218d6c036ec03c8c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57071ad9a63546e29515ff2caa042425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cebbd632fc42c9b996ca94cc08f8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b344d1ffa1604f50b1d34aa1f7dce22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d2a544360b4faba28bfcaef5c9f3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4c0d13e33847e2a9cb1d67a0c7de19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "\n",
    "semb_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "xenc_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11753,
     "status": "ok",
     "timestamp": 1716557764763,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "r5EKXcV1sxbg",
    "outputId": "3e801987-a143-4207-8436-e821b22275bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51798"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_conversations(conversations):\n",
    "\n",
    "    # Convert conversation data into a list of message-response pairs.\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(conversations)):\n",
    "        conv = conversations[i]\n",
    "        for j in range(len(conv)):\n",
    "            if conv[j]['from'] == 'human':\n",
    "                message = conv[j]['value']\n",
    "            if conv[j]['from'] == 'gpt':\n",
    "                response = conv[j]['value']\n",
    "        result.append({\"message\": message, \"response\": response})\n",
    "    return result\n",
    "\n",
    "size = int(len(dataset) * 0.1)\n",
    "\n",
    "reduced_dataset_for_retrieval = dataset[:size]['conversations']\n",
    "\n",
    "conversation_pairs = convert_conversations(reduced_dataset_for_retrieval)\n",
    "len(conversation_pairs)  # Number of message-response pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 112092,
     "status": "ok",
     "timestamp": 1716557881680,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "quc6qB9Lsxbg",
    "outputId": "490c00c7-ef2c-46c5-87a7-f7106df6223b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd9b38f2c41fea2fc5cfe962194b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1619 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate embeddings for messages in conversation pairs using a SentenceTransformer model.\n",
    "corpus_embeddings = semb_model.encode([sample['message'] for sample in conversation_pairs],\n",
    "                                      convert_to_tensor=True,\n",
    "                                      show_progress_bar=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94448,
     "status": "ok",
     "timestamp": 1716558008619,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "HMOOzCxAsxbg",
    "outputId": "c8a27e68-ff4d-4529-a34a-864b484ec45f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating HNSWLIB index\n",
      "Saving index to: ./retrieval_based/emp_dialogue_hnswlib.index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hnswlib\n",
    "\n",
    "# Create empty index\n",
    "hnswlib_index = hnswlib.Index(space='cosine', dim=corpus_embeddings.size(1))\n",
    "\n",
    "# Define hnswlib index path\n",
    "index_path = \"./retrieval_based/emp_dialogue_hnswlib.index\"\n",
    "\n",
    "# Load index if available\n",
    "if os.path.exists(index_path):\n",
    "    print(\"Loading index...\")\n",
    "    hnswlib_index.load_index(index_path)\n",
    "# Else index data collection\n",
    "else:\n",
    "    # Initialise the index\n",
    "    print(\"Start creating HNSWLIB index\")\n",
    "    hnswlib_index.init_index(max_elements=corpus_embeddings.size(0), ef_construction=400, M=64)\n",
    "    #  Compute the HNSWLIB index (it may take a while)\n",
    "    hnswlib_index.add_items(corpus_embeddings.cpu(), list(range(len(corpus_embeddings))))\n",
    "    # Save the index to a file for future loading\n",
    "    print(\"Saving index to:\", index_path)\n",
    "    hnswlib_index.save_index(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WR1022WIsxbg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_response(message, mes_resp_pairs, index, re_ranking_model=None, top_k=32):\n",
    "    message_embedding = semb_model.encode(message, convert_to_tensor=True).cpu()\n",
    "\n",
    "    corpus_ids, _ = index.knn_query(message_embedding, k=top_k)\n",
    "\n",
    "    model_inputs = [(message, mes_resp_pairs[idx]['response']) for idx in corpus_ids[0]]\n",
    "    cross_scores = xenc_model.predict(model_inputs)\n",
    "\n",
    "    idx = np.argsort(-cross_scores)[0]\n",
    "\n",
    "    return mes_resp_pairs[corpus_ids[0][idx]]['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 11363,
     "status": "ok",
     "timestamp": 1716558038479,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "45iMM2lisxbg",
    "outputId": "712cfe96-188e-428c-e3d6-fcc875f9c9fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which are the primary colors?\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'To create purple on a computer screen, we mix red and blue light at equal intensity on a black screen. These are two of the additive primary colors in the RGB color model.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = input()\n",
    "\n",
    "chatbot_response = get_response(\n",
    "    prompt, conversation_pairs, hnswlib_index, re_ranking_model=xenc_model\n",
    ")\n",
    "chatbot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "executionInfo": {
     "elapsed": 56352,
     "status": "ok",
     "timestamp": 1716558111291,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "ZO8xx5qg1Tfd",
    "outputId": "a62e5064-82e3-4c2f-b333-d55ac34c3713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you explain the concept of meme to someone of the 18th century?\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"The concept of measurement that did not exist in Benjamin Franklin's time is the standardization of time. During the 18th century, people in Europe did not adhere to precise schedules, and there was no standardized time system. However, this changed as rail and communication networks developed, requiring a standardized time system for efficient functioning. Although Franklin did not propose daylight saving time (DST) specifically, his suggestion to economize on candles by rising earlier to utilize morning sunlight reflects an understanding of the need to organize time more effectively.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = input()\n",
    "\n",
    "chatbot_response = get_response(\n",
    "    prompt, conversation_pairs, hnswlib_index, re_ranking_model=xenc_model\n",
    ")\n",
    "chatbot_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ne4UpHQKsxbg"
   },
   "source": [
    "## Some considerations\n",
    "\n",
    "1. **Corpus Selection**: The success of a retrieval-based model heavily depends on the quality and diversity of the training data. Curating a corpus that mirrors the breadth of topics and linguistic nuances present in ChatGPT's responses is crucial.\n",
    "\n",
    "2. **Response Appropriateness**: Ensuring that the retrieval-based model outputs only appropriate responses requires diligent evaluation and validation. Incorporating human-in-the-loop feedback loops or leveraging external validation datasets can help assess the relevance and coherence of generated responses.\n",
    "\n",
    "3. **Handling Ambiguity**: Ambiguity is inherent in natural language, and addressing it is a significant challenge for retrieval-based chatbots.\n",
    "\n",
    "4. **Safety and security**: Protection against the generation of inappropriate or harmful content is easily controlled since we already know the set of possible answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "071gzao-sxbg"
   },
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Gm1Le58sxbh"
   },
   "source": [
    "## 1. DistilGPT2\n",
    "\n",
    "[DistilGPT2](https://huggingface.co/distilbert/distilgpt2) (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxemVyR-sxbh"
   },
   "outputs": [],
   "source": [
    "name = 'distilbert/distilgpt2'\n",
    "\n",
    "# Directory where the fine-tuned model will be saved\n",
    "DistilGPT2_SAVE_DIRECTORY = SAVE_DIRECTORY + 'distilGPT2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zbtp-gECsxbh"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForCausalLM.from_pretrained(name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq6v-35zfdjJ"
   },
   "source": [
    "### Let's play a bit with the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15168,
     "status": "ok",
     "timestamp": 1716619969914,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "5O3AiEqbfh0w",
    "outputId": "4bad2896-cf95-4608-f686-c490beb97717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary colors are:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The primary colors are:\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The primary colors are: red, green, blue, yellow, orange, purple, and yellow.\n",
      "\n",
      "The colors of the colors in this article are based on the color of each color. For example, red is the most common color in the U.S. and the second most commonly used color for the United States.\n"
     ]
    }
   ],
   "source": [
    "prompt = input()\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0R7WSgp5l3ef"
   },
   "source": [
    "They're not exactly primary colors, but the response seems consistent with the request. Now we've used the typical structure a Causal Language Model is trained on, which involves completing a sentence. Let's try some questions instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10843,
     "status": "ok",
     "timestamp": 1716620012494,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "_DBjkx00hnbQ",
    "outputId": "bcf8cc0e-caf0-4795-8d08-37ec8bfe90c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which are the primary colors?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Which are the primary colors?\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Which are the primary colors?\n",
      "\n",
      "I’m not going to say that I don't like the color scheme, but I do like it. I think it‪s a good idea to use a lot of different colors to make it look good.\n",
      "What do you think is the best way to do this? Do you have any suggestions?\n"
     ]
    }
   ],
   "source": [
    "prompt = input()\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boUV6lNhmfoW"
   },
   "source": [
    "Alright, GPT-2 doesn't seem to want to cooperate. But as we've previously observed, it does know the answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9590,
     "status": "ok",
     "timestamp": 1716620846132,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "1PdZh1iHksKm",
    "outputId": "551ee6cd-812d-4117-eb4b-eefa3976cc1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of italy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of italy?\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "What is the capital of italy?”\n",
      "\n",
      "“I’m not sure, but I don't think it's a big deal. It's just a matter of time before we get to the point where we're going to be able to do it again. I think we'll have to figure out a way to make it happen.‡\n"
     ]
    }
   ],
   "source": [
    "prompt = input()\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQoQBz20moZ_"
   },
   "source": [
    "Alright, this isn't working at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sl0_CrSglKLP"
   },
   "source": [
    "Why? DistilGPT-2 is a powerful language model that can generate human-like text based on the input it receives. However, there is a significant difference in performance when comparing the plain, pre-trained GPT-2 to a version that has been fine-tuned to function as a chatbot\n",
    "\n",
    "   - **Plain GPT-2**: It is more of a generalist, suitable for a wide range of text generation tasks but not optimized for any particular one. Its versatility comes at the cost of performance in specific scenarios like answering questions effectively.\n",
    "   - **Fine-Tuned GPT-2**: This model becomes more of a specialist, adapting its responses to suit the specific needs of a chatbot or an instructional agent. The fine-tuning process equips it with the skills to handle user queries more adeptly and provide clear, concise, and helpful answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5tUzMUOyCkN"
   },
   "source": [
    "### Let's see the effects of fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9uIVNIXsxbh"
   },
   "source": [
    "Check if the tokenizer has a pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1716558151288,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "U4LPP3Ajsxbh",
    "outputId": "c0db82e5-efac-407d-f755-09f5e758e2bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qpEGBbqsxbh"
   },
   "source": [
    "Since the tokenizer does not have a PAD token, we set it equal to the EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp9sfMftsxbh"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gztTE9kPsxbh"
   },
   "source": [
    "Let's check the number of parameters in our model. In this initial version, we are training all the parameters. We will explore other approaches later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1716558159237,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "417zCZlysxbh",
    "outputId": "f5aa9578-7c42-4566-e81f-e41d7a01e1e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters in the model: 81912576\n",
      "The number of trainable parameters in the model: 81912576\n"
     ]
    }
   ],
   "source": [
    "# Initialize a variable to store the count of trainable parameters\n",
    "total_parameters = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "print(f'The total number of parameters in the model: {total_parameters}')\n",
    "print(f'The number of trainable parameters in the model: {trainable_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYabESYCsxbh"
   },
   "source": [
    "We systematically process every conversation in the dataset. Upon analysis, we observed that certain conversations lack a system prompt. Consequently, we decided to exclude the system prompt during the parsing process. In the parsing function, we utilize the `eos_token` to separate system prompt, human and GPT strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvJ7yKGEsxbi"
   },
   "outputs": [],
   "source": [
    "def parse_conversation(conv, eos_token):\n",
    "\n",
    "    # Initialize the output string with the 'system' label\n",
    "    out = SYSTEM_TOKEN\n",
    "    # Check if the first message is not from 'system'\n",
    "    if conv[0]['from'] != 'system':\n",
    "        # If the first message is not from 'system', append only the end-of-segment token\n",
    "        out += eos_token + HUMAN_TOKEN + conv[0]['value'] + eos_token + GPT_TOKEN +  conv[1]['value'] + eos_token\n",
    "    else:\n",
    "        # If the first message is from 'system', format and append the system, human, and gpt messages with the end-of-segment token\n",
    "        out += conv[0]['value'] + eos_token + HUMAN_TOKEN + conv[1]['value'] + eos_token + GPT_TOKEN + conv[2]['value'] + eos_token\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1716558413743,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "uP6pQNtBsxbi",
    "outputId": "f56c9740-0cec-43ba-da63-a0a9fa428f67"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'system: You are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|endoftext|>human: Arvoisa puhemies, 14. elokuuta tapahtui vakava onnettomuus vaalipiirini Castilla-la Manchan alueella Puertollanossa sijaitsevassa Repsol YPF:n omistamassa öljynjalostamossa, joka on yksi Espanjan tärkeimmistä öljynjalostamoista.\\n\\nCould you please translate this to English?<|endoftext|>gpt: Dear Speaker, on August 14th, a serious accident occurred in my constituency within the Castilla-la Mancha region at the Repsol YPF-owned oil refinery located in Puertollano, which is one of the most important oil refineries in Spain.<|endoftext|>'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_conversation(reduced_dataset[0]['conversations'], tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JStR_nXAsxbi"
   },
   "source": [
    "Let's parse all conversations in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SvMLgFDsxbi"
   },
   "outputs": [],
   "source": [
    "parsed_dataset = [parse_conversation(reduced_dataset[i]['conversations'], tokenizer.eos_token) for i in range(len(reduced_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUtZbJ5nsxbi"
   },
   "outputs": [],
   "source": [
    "assert len(parsed_dataset) == len(reduced_dataset) # Check that we haven't missed any conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1716558461421,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "FcQeDXdnsxbi",
    "outputId": "5eeaa8aa-a189-47e0-b8b0-6e6d624b2249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 3625\n",
      "Validation set size: 776\n",
      "Test set size: 778\n"
     ]
    }
   ],
   "source": [
    "# Divide the list in train, validation and test set\n",
    "train_size = int(len(reduced_dataset) * 0.7)\n",
    "val_size = int(len(reduced_dataset) * 0.15)\n",
    "\n",
    "train_set = parsed_dataset[:train_size]\n",
    "val_set = parsed_dataset[train_size:train_size+val_size]\n",
    "test_set = parsed_dataset[train_size+val_size:]\n",
    "\n",
    "print(\"Train set size:\", len(train_set))\n",
    "print(\"Validation set size:\", len(val_set))\n",
    "print(\"Test set size:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdrG-S15sxbi"
   },
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({'text': train_set})\n",
    "valid_data = Dataset.from_dict({'text': val_set})\n",
    "test_data = Dataset.from_dict({'text': test_set})\n",
    "\n",
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['validation'] = valid_data\n",
    "data['test'] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKesK_Pjsxbi"
   },
   "source": [
    "Let's define a function to tokenize conversations. In this initial function, we'll use only input_ids, without incorporating attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 11844,
     "status": "ok",
     "timestamp": 1716558480611,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "Yjr5e1wAsxbi",
    "outputId": "802c763d-3b5a-40f2-84d8-fe1242be2655"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e978cafdcb8f446d9708651e21de58c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9beda61b0284444ebedf3d3409dd889d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/776 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b318d6699f452f979ad9775daeaf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/778 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the input text, ensuring that sequences are padded and truncated to fit the model's requirements\n",
    "    input_encodings = tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "    # Create a dictionary with 'input_ids' from the tokenized encodings\n",
    "    sample = {\n",
    "        'input_ids': input_encodings.input_ids\n",
    "    }\n",
    "\n",
    "    # Return the dictionary containing the input_ids\n",
    "    return sample\n",
    "\n",
    "# Apply the tokenize_function to the entire dataset in batches, returning the tokenized data\n",
    "tokenized_data = data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Creatng the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S4WhJXQsxbm"
   },
   "source": [
    "Define the training arguments and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tQnRPEZsxbm"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"gpt2_trainer\",\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,  # Gradient accumulation to use batch sized larger the available VRAM\n",
    "    learning_rate=6.25e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    fp16=True  # Mixed precision training, to speed up and save memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "executionInfo": {
     "elapsed": 1212957,
     "status": "ok",
     "timestamp": 1716559710303,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "OuIUOvRDsxbm",
    "outputId": "5e4b2383-bcfb-4333-de32-aa66634bbdd7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [339/339 20:05, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=339, training_loss=2.5736652925654497, metrics={'train_runtime': 1210.2912, 'train_samples_per_second': 8.985, 'train_steps_per_second': 0.28, 'total_flos': 2832979369918464.0, 'train_loss': 2.5736652925654497, 'epoch': 2.990077177508269})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17825,
     "status": "ok",
     "timestamp": 1716559823423,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "nAaUoAF6sxbm",
    "outputId": "f61d48ad-3074-4d58-fc34-754500c2602b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Models/distilGPT2/tokenizer_config.json',\n",
       " './Models/distilGPT2/special_tokens_map.json',\n",
       " './Models/distilGPT2/vocab.json',\n",
       " './Models/distilGPT2/merges.txt',\n",
       " './Models/distilGPT2/added_tokens.json',\n",
       " './Models/distilGPT2/tokenizer.json')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(DistilGPT2_SAVE_DIRECTORY)\n",
    "tokenizer.save_pretrained(DistilGPT2_SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "463KOZYMsxbm"
   },
   "source": [
    "### Load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugM-Dclrsxbm"
   },
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt, eos_token, system_prompt = None):\n",
    "    # Set a default system prompt if none is provided\n",
    "    if system_prompt is None:\n",
    "        system_prompt = 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.'\n",
    "\n",
    "    # Construct the full prompt string by combining the system prompt, end-of-sequence token, user prompt, and another end-of-sequence token\n",
    "    result = 'system: ' + system_prompt + eos_token + 'human: ' + prompt + eos_token + 'gpt: '\n",
    "\n",
    "    # Return the constructed prompt string\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWB5iMK8sxbm"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(DistilGPT2_SAVE_DIRECTORY)\n",
    "model = AutoModelForCausalLM.from_pretrained(DistilGPT2_SAVE_DIRECTORY).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYu-u7Htolh3"
   },
   "source": [
    "### Exploring Decoding Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1716623563604,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "q5GbZ1pbq1pf",
    "outputId": "93ffc41c-727a-40ec-963b-255c64e83cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: system: You are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|endoftext|>human: What is the capital of Italy?<|endoftext|>gpt: \n"
     ]
    }
   ],
   "source": [
    "prompt = 'What is the capital of Italy?'\n",
    "\n",
    "prompt = prepare_prompt(prompt, tokenizer.eos_token)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYmsi6L4o1z1"
   },
   "source": [
    "#### Greedy decoding\n",
    " The simplest approach involves selecting the most probable token at each step and feeding it as the input for the next step. However, this method often produces unsatisfactory results, such as repetitive or uninformative responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3227,
     "status": "ok",
     "timestamp": 1716623570571,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "GvyMhTYbo5V3",
    "outputId": "b14da059-9cf7-41ce-a305-08e2782ad0a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "system: You are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|endoftext|>human: What is the capital of Italy?<|endoftext|>gpt:  Italian: The capital of Italy is the capital of Italy. It is the capital of the Republic of Italy. It is the capital of the Republic of Italy.\n",
      "\n",
      "The capital of Italy is the capital of the Republic of Italy. It is the capital of the Republic of Italy.\n",
      "\n",
      "The capital of Italy is the capital of the Republic of Italy. It is the capital of the Republic of Italy.\n",
      "\n",
      "The capital of Italy is the capital of the Republic of Italy. It is the capital of the Republic of Italy.\n",
      "\n",
      "The capital of Italy is the capital of the Republic of Italy. It is the capital of the Republic of Italy.\n",
      "\n",
      "The capital of Italy is the capital of the Republic of Italy. It is the capital of the Republic of Italy.\n",
      "\n",
      "The capital of Italy is the capital of the Republic of Italy. It is the capital of the Republic of Italy.\n",
      "\n",
      "The capital of Italy is the capital of the Republic of Italy. It is the\n"
     ]
    }
   ],
   "source": [
    "# generate text until the generated output length reaches 200 tokens\n",
    "greedy_output = model.generate(input_ids, max_new_tokens=200)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFcEt_H7p7Rm"
   },
   "source": [
    "#### Beam Search\n",
    "Beam search decoding strategy selects the most probable sequence of tokens by exploring multiple paths simultaneously, retaining a fixed number of top candidates at each step to maximize the likelihood of generating coherent output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3694,
     "status": "ok",
     "timestamp": 1716623991013,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "Al7hPAn3p-jA",
    "outputId": "c8eea70a-13ca-4658-9a6c-f58a54073935"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy. The capital of Italy is Italy.\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0, input_ids.size(1):], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2774,
     "status": "ok",
     "timestamp": 1716623985423,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "4RvlKlZJrgZt",
    "outputId": "447baf85-927c-4e3d-bae9-794830f8d186"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " Italy is a country with a population of more than 1,000 people. It is home to the largest city in the world. The capital is Rome.\n",
      "\n",
      "The capital, Rome, is one of the most populous cities in Europe. Its population is estimated to be around 1.5 million people, making it the second-biggest city of all time, according to a new study from the University of California, Santa Barbara, which found that Italy has the highest number of births per woman per year. In the United States, the city is ranked number one in terms of population and population, while in other European countries, it is number two. Italy's population stands at 2.6 million. This means that it's the third-largest city on the planet, behind only the Czech Republic and Slovakia, and the fourth-most populous country on Earth. According to data provided by the U.S. Census Bureau, Italy ranks second in world, followed by Switzerland, Switzerland and Switzerland.\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0, input_ids.size(1):], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1635,
     "status": "ok",
     "timestamp": 1716623973884,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "UUudJfMrtBsO",
    "outputId": "bacc8db8-a00b-4eaf-d72f-8beb9a9972cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0:  Italy is a country with a population of more than 1,000 people. It is home to the largest city in the world. The capital is Rome.\n",
      "\n",
      "The capital, Rome, is one of the most populous cities in Europe. Its population\n",
      "1:  Italy is a country with a population of more than 1,000 people. It is home to the largest city in the world. The capital is Rome.\n",
      "\n",
      "The capital, Rome, is one of the most populous cities in Italy. Its population\n",
      "2:  Italy is a country with a population of more than 1,000 people. It is home to the largest city in the world. The capital is Rome.\n",
      "\n",
      "The capital, Rome, is one of the most important cities in Italy, and it\n",
      "3:  Italy is a country with a population of more than 1,000 people. It is home to the largest city in the world. The capital is Rome.\n",
      "\n",
      "The capital, Rome, is one of the most important cities in Italy, and the\n",
      "4:  Italy is a country with a population of more than 1,000 people. It is home to the largest city in the world. The capital is Rome.\n",
      "\n",
      "The capital, Rome, is one of the most important cities in Italy, with its\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output[input_ids.size(1):], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ap6kzkcgtQs1"
   },
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1330,
     "status": "ok",
     "timestamp": 1716623961171,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "7zmDgCLutToG",
    "outputId": "58a3ad1d-5abc-44b2-9d6e-12f60662a43e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Italy is composed of four branches of the Roman Empire and comprises some 5,000 countries. It comprises Italy's 6th most populous country, divided by one part of the Republic of Modena province. Italy was first mentioned by Mr Perredelli in\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0, input_ids.size(1):], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1716623957150,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "JnnflcVCvu-d",
    "outputId": "cb7505c0-92e9-4569-d8c8-baa56eb8d2e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " Italy is the capital of Italy, and it is the capital of Italy, which is not mentioned in the article.\n",
      "\n",
      "The capital of the Italian state is Italy, and it is not mentioned in the article. The capital of Italy is a city\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_k=0,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0, input_ids.size(1):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FezDWcNXv4-8"
   },
   "source": [
    "#### Tok-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1716623952083,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "vZn8T5tZv7tG",
    "outputId": "01a3149d-67b8-4804-9f72-cf839f973ae3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Italy is composed of four countries: Britain, Italy and Austria; Italy is called the Mediterranean, with its border with France (Belgium) and the Mediterranean Sea, and is under its control. Italy was first mentioned by Napoleon of France a century\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results.\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0, input_ids.size(1):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JF_mx-yswEEX"
   },
   "source": [
    "#### Top-p (nucleus) sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1068,
     "status": "ok",
     "timestamp": 1716623945263,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "RRKQ2ZPxwHct",
    "outputId": "23075843-8dc9-48b9-88d6-39b1a8622335"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Italy is composed of four branches of the Roman Empire and comprises some 5,000 countries. It comprises Italy's 6th most populous country, divided by one part of the Republic of Modena, in the northeastern provinces of Serif and Corriere\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results.\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0, input_ids.size(1):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2496,
     "status": "ok",
     "timestamp": 1716623927582,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "WjMaPYD0wT-d",
    "outputId": "a2df04d1-c777-4175-d403-a1c8077ab0b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Italy is the capital of Italy, and this city is the capital, making it the third-largest city in the country (18,200). The capital is known for its beauty and its historic architecture. However, it has faced many challenges due to\n",
      "1:  This is the capital of Italy. It has a rich history, and is home to one of the greatest cities in history.\n",
      "\n",
      "A city has a rich history: The city is founded as a museum in 1798 by the wealthy architect Giuse\n",
      "2:  \n",
      "In the following article, Italian and its capital is Naples. Italy is a city of around 4,000 people. The capital has approximately 4,800 square miles (3,520 square km) of land in its heart.\n",
      "\n",
      "[\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results.\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output[input_ids.size(1):], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x8jSIOLsxbm"
   },
   "source": [
    "### Testing: let's ask something to first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14407,
     "status": "ok",
     "timestamp": 1716624042783,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "rjs-xzUusxbn",
    "outputId": "7e21711b-ba71-41ea-9773-45d1097a4a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which are the primary colors?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: system: You are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|endoftext|>human: Which are the primary colors?<|endoftext|>gpt: \n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "system: You are an AI assistant. You will be given a task. You must generate a detailed and long answer.human: Which are the primary colors?gpt: \n",
      "\n",
      "The main colors are: red, green, blue, and yellow. The colors of the two colors vary by color, with the most common colors being the blue and green. These colors can vary from one color to the next, depending on the color of your choice. For example, if you are looking for a color that has a lot of colors, you might want to look for it in the shade of a green or a blue. If you do not have enough color options, choose the one that is most suitable for you. Your choice is based on your preferences, but it is not possible to tell which color is the best option. Choose from:\n",
      "(1). green\n",
      "2). blue\n",
      "3). purple\n",
      "4). red\n",
      "5). orange\n",
      "6). yellow\n",
      "7).\n"
     ]
    }
   ],
   "source": [
    "prompt = prepare_prompt(input(), tokenizer.eos_token)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40278,
     "status": "ok",
     "timestamp": 1716624210013,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "Ml6HEy-l8buM",
    "outputId": "2ac56325-10b4-4a1b-ae76-a8d8e9914569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me the steps to make pizza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: system: You are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|endoftext|>human: Give me the steps to make pizza<|endoftext|>gpt: \n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "system: You are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|endoftext|>human: Give me the steps to make pizza<|endoftext|>gpt:  Instructions: In this task, you are given the following information:\n",
      "\n",
      "1. Identify the ingredients in the pizza.\n",
      "2. Determine if there are any ingredients that are missing from the recipe. \n",
      "3. If there is a missing ingredient that is missing, add the correct ingredients to the list to ensure that it is not present in a recipe that contains the missing ingredients. This way, it can be easily identified by looking at the ingredient list and comparing it to other ingredients found in other recipes. For example, if you have a pizza that does not contain the same ingredients as the original recipe, then you can compare it with the new recipe to see if it meets the criteria of the given recipe or not. In the case of pizza, the answer is: \"No\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = prepare_prompt(input(), tokenizer.eos_token)#, system_prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0], skip_special_tokens=False)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jn0uXx-Hsxbn"
   },
   "source": [
    "### Some considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQlhPyvesxbn"
   },
   "source": [
    "#### About fine-tuning\n",
    "\n",
    "Training DistilGPT-2 to become a chatbot by updating all its parameters can present several challenges and limitations, which can hinder its performance and effectiveness.\n",
    "\n",
    "1. **Computational Cost**:\n",
    "   - Training all parameters of a model, even a distilled version like DistilGPT-2, is computationally expensive. It requires significant GPU resources and time, which might not be feasible for all organizations or projects. This high computational cost can also limit the frequency and extent of experimentation.\n",
    "\n",
    "2. **Catastrophic Forgetting**:\n",
    "   - Updating all parameters can lead to **catastrophic forgetting**, where the model loses previously learned knowledge while learning new information. This is especially problematic if the model needs to retain certain foundational knowledge while being fine-tuned for specific tasks.\n",
    "\n",
    "3. **Suboptimal Fine-Tuning**:\n",
    "   - Fine-tuning all parameters might not be the most efficient approach. Often, only specific layers or parts of the model need fine-tuning to adapt to a new task effectively. By training the entire model, we might be unnecessarily modifying parts that don't significantly contribute to the chatbot's performance, leading to inefficiencies.\n",
    "\n",
    "Given these considerations, it becomes clear that training all parameters of DistilGPT-2 to function as a chatbot might not yield the best results. Alternative approaches, such as training only specific layers, using transfer learning, or employing techniques like adapters or low-rank adaptation (LoRA), might offer more efficient and effective solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpB-88mHsxbn"
   },
   "source": [
    "#### About dataset preprocessing\n",
    "\n",
    "Fine-tuning our model using the end-of-sequence (eos) token approach encountered several issues, prompting us to switch to using newline characters (`'\\n'`) instead.\n",
    "\n",
    "1. **Token Confusion**:\n",
    "   - The eos_token was initially intended to signal the end of a sequence. When used within prompts to separate different parts (system prompt, user input, model output), the model sometimes misinterpreted it as the end of the entire input, leading to incomplete or cut-off responses.\n",
    "\n",
    "2. **Training Instability**:\n",
    "   - Incorporating the eos_token within prompts caused instability during fine-tuning. The model often struggled to learn clear boundaries between the system instructions, user input, and its responses, resulting in mixed or unpredictable outputs.\n",
    "\n",
    "To address these issues, we decided to use newline characters (`'\\n'`) as separators instead of the eos_token. This change brought several improvements:\n",
    "\n",
    "1. **Clear Separation**:\n",
    "   - Newline characters provided a clearer and more intuitive way to separate different parts of the prompt. The model could easily distinguish between the system prompt, user input, and its own output, leading to more accurate and coherent responses.\n",
    "\n",
    "2. **Improved Training Stability**:\n",
    "   - The model adapted more effectively during fine-tuning with newline characters, as they did not carry the same end-of-sequence implications as eos_token. This resulted in a more stable training process and better overall performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ20kzVcsxbn"
   },
   "source": [
    "# 2. GPT2 - Large. Fine-tuning using LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQQ99-nLsxbn"
   },
   "source": [
    "Fine-tuning a model using Low-Rank Adaptation (LoRA) offers several advantages that make it a compelling approach for adapting pre-trained language models to specific tasks.\n",
    "1. **Efficiency in Parameter Updates**\n",
    "   - **Reduced Parameter Count**: LoRA introduces a small set of additional parameters that represent low-rank updates to the pre-trained model's weight matrices. This significantly reduces the number of parameters that need to be trained compared to fine-tuning the entire model.\n",
    "   - **Memory Efficiency**: Because fewer parameters are updated, LoRA requires less memory, making it possible to fine-tune large models on hardware with limited resources, such as consumer-grade GPUs.\n",
    "\n",
    "2. **Preservation of Pre-trained Knowledge**\n",
    "   - **Minimal Interference**: LoRA ensures that the core knowledge embedded in the pre-trained model remains largely untouched. By applying low-rank updates, it adapts the model to new tasks without overwriting the essential features learned during pre-training.\n",
    "   - **Reduced Catastrophic Forgetting**: Since only a small portion of the model's parameters are altered, the risk of catastrophic forgetting—where the model loses previously acquired knowledge—is minimized.\n",
    "\n",
    "3. **Faster Training and Convergence**\n",
    "   - **Quicker Adaptation**: Due to the reduced number of trainable parameters, models fine-tuned with LoRA often converge faster than those fine-tuned in a traditional manner. This leads to shorter training times and quicker deployment cycles.\n",
    "   - **Hyperparameter Tuning**: With fewer parameters to optimize, hyperparameter tuning becomes more straightforward and less computationally intensive.\n",
    "\n",
    "Using LoRA, we were able to fine-tune a larger model to become a chatbot efficiently, as it requires updating only a **small subset of parameters**, preserving the model's core knowledge while adapting it to specific conversational tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJHnJK9Osxbn"
   },
   "source": [
    "[GPT-2 Large](https://huggingface.co/openai-community/gpt2-large) is the 774M parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtodCNcJsxbo"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'openai-community/gpt2-large'\n",
    "GPT2_LoRA_SAVE_DIRECTORY = SAVE_DIRECTORY + 'GPT2_LoRA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epbgh2Bdsxbo"
   },
   "source": [
    "We decided to use quantization in 4 bits because we aimed to fine-tune a larger and more powerful model than DistilGPT-2, which unfortunately did not fit within the GPU resources available on Colab.\n",
    "\n",
    "Indeed, quantization offers several benefits, especially when dealing with resource-constrained environments or large models.\n",
    "\n",
    "1. **Reduced Memory Footprint**:\n",
    "   - Quantization reduces the memory required to store model parameters by representing them with fewer bits.\n",
    "\n",
    "2. **Scalability**:\n",
    "   - Quantization allows for scaling up model deployment to a larger user base or a wider range of devices without significantly increasing computational or memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0JKpJ7vsxbo"
   },
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 238937,
     "status": "ok",
     "timestamp": 1716624572761,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "A1Jc21pM92eM",
    "outputId": "0779f14d-c846-4627-c6da-fb1419afbb1e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f8439d4fcd49e5bac150958953c4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7becb3e6a346417683daad526977ccaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39ce5f66d884be9a2f45318ab6b52b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfa68064fa94d5d862f8ca3c08d1779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b139792448e4b2fada44a32fe4a1751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3fb64fba644e5e960cce9a57a8afbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d034983362490b9560416eec845b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device_map = 'auto'\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,\n",
    "                                             device_map=device_map,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,\n",
    "                                          trust_remote_code=True,\n",
    "                                          padding_side=\"left\",\n",
    "                                          add_eos_token=True,\n",
    "                                          add_bos_token=True,\n",
    "                                          use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aY_wEj85ydBV"
   },
   "source": [
    "### Let's play a bit with the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22873,
     "status": "ok",
     "timestamp": 1716624653012,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "LduMyL0syi-t",
    "outputId": "27d25c45-6dbc-4c88-dc67-30859aeda473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary colors are:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The primary colors are:\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The primary colors are: red, green, blue, yellow, orange, purple, white, and black.\n",
      "\n",
      "The secondary colors include: cyan, magenta, brown, black, gray, silver, gold, copper, bronze, indigo, turquoise, teal, violet, red-orange, cyan-violet, pink-red, light-blue-purple, dark-green-yellow, pale-pink-white, deep-gray-black, grey-brown, olive-beige, navy-grey, beige-cream, mustard-cinnamon, cinnamon-saffron, saffron-lavender, lavender-rose, lemon-lime, lime-peppermint, peppermint-honeydew, rose-bud, bergamot-lemon, lemongrass-lilac, lily-of-the-valley, lilac-moss, mauve\n"
     ]
    }
   ],
   "source": [
    "prompt = input()\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C23vEoNrzfPd"
   },
   "source": [
    "They're not exactly primary colors, but the response seems consistent with the request. Now we've used the typical structure a Causal Language Model is trained on, which involves completing a sentence. Let's try some questions instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12559,
     "status": "ok",
     "timestamp": 1716624769413,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "Yr_MgNTGzwgN",
    "outputId": "328ef2c6-12f2-4aaa-b9e2-a0736f4eed99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of italy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of italy?\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "What is the capital of italy?\n",
      "\n",
      "The capital city of Italia is Rome. It is located in the south-eastern part of the country. The city has a population of around 1.5 million people, and is one of Italy's most popular tourist destinations.\n"
     ]
    }
   ],
   "source": [
    "prompt = input()\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34622,
     "status": "ok",
     "timestamp": 1716625016686,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "wkySEaOC0TsO",
    "outputId": "05c31193-1713-4f69-bc0c-7e0b72984d83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me the steps to make pizza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Give me the steps to make pizza\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Give me the steps to make pizza and I'll make it for you.\n",
      "\n",
      "I've been making pizza for a long time now, and it's one of my favorite things to do with my family. It's a great way to spend time with friends, family, or just have a good time. I've even made it at home for my mom, who is a huge fan of pizza. She loves it so much that she's even been known to eat it on the go! I'm not sure how she does it, but she seems to like it a lot more than I do, so I guess that's just the way it is.\n",
      "\n",
      "\n",
      "This recipe is so easy to follow, you'll be making it in no time at all. The only thing you really need to know is how much dough you need, how long you want it to sit in the fridge, what kind of toppings you're going to use, etc. If you've never made pizza before, this is the perfect recipe to get you started. You can also make this recipe ahead of time and freeze it. Just pop it into the freezer for up to a week, then thaw it out and bake it the next day. This way you can have it ready to go when you get home from work or school and you don't have to worry about it getting cold.\n"
     ]
    }
   ],
   "source": [
    "prompt = input()\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=500,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szqz9Bxmz6du"
   },
   "source": [
    "Oh wow! GPT-2 Large is truly remarkable. Despite being quantized for memory and computational efficiency, it showcases an extraordinary capability to adeptly handle questions and tasks it hasn't been explicitly trained on.\n",
    "\n",
    "Maybe we can do it better. It could answer the question directly without getting lost in chatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwdozGpczgQ7"
   },
   "source": [
    "### Let's see the effects of fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oCjrWZx98Qb"
   },
   "source": [
    "The model we are using now is much larger than the previous one. Let's see how many parameters it has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1716560326520,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "s0-aEh3A94ak",
    "outputId": "e840ca1d-b53e-48ee-b165-9e0da1d1bf3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters in the model: 420135680\n"
     ]
    }
   ],
   "source": [
    "# Initialize a variable to store the count of trainable parameters\n",
    "total_parameters = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "print(f'The total number of parameters in the model: {total_parameters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1828Z1Resxbo"
   },
   "source": [
    "In the previous model, we encountered issues when setting the `pad_token` equal to the `eos_token`. This configuration caused confusion during text generation, as the model struggled to determine when to stop generating text accurately. While initially convenient for managing padding during tokenization, this approach resulted in ambiguity regarding the end of text generation.\n",
    "\n",
    "To resolve this issue, we opted to introduce a new token, `[PAD]`, both in the tokenizer and within the model itself. This decision provided a clearer distinction between padding and the end of text generation, ensuring more accurate and reliable model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1716560345032,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "Y27D0sohsxbp",
    "outputId": "da40604e-c52d-448e-92a4-f3bb93393292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer has 50257 tokens\n",
      "After adding the pad token, the tokenizer has 50258 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"The tokenizer has {len(tokenizer)} tokens\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "  with torch.no_grad():\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "  model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "  print(f\"After adding the pad token, the tokenizer has {len(tokenizer)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrMhE1Ojsxbp"
   },
   "source": [
    "Let's define the new `parse_conversation` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNA6oARTsxbp"
   },
   "outputs": [],
   "source": [
    "def parse_conversation(conv, eos_token):\n",
    "    # Initialize the output string with the system token\n",
    "    out = SYSTEM_TOKEN\n",
    "\n",
    "    # Check if the first message in the conversation is from the user or system\n",
    "    if conv[0]['from'] != 'system':\n",
    "        # If the first message is from the user, append the human token, user input, and model response\n",
    "        out += '\\n' + HUMAN_TOKEN + conv[0]['value'] + '\\n' + GPT_TOKEN +  conv[1]['value'] + eos_token\n",
    "    else:\n",
    "        # If the first message is from the system, append the system response, human input, and model response\n",
    "        out += conv[0]['value'] + '\\n'+ HUMAN_TOKEN + conv[1]['value'] + '\\n' + GPT_TOKEN +  conv[2]['value'] + eos_token\n",
    "\n",
    "    # Return the parsed conversation\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wamjoPensxbp"
   },
   "outputs": [],
   "source": [
    "parsed_dataset = [parse_conversation(reduced_dataset[i]['conversations'], tokenizer.eos_token) for i in range(len(reduced_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1716560433022,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "SR9wRKjksxbp",
    "outputId": "8fb13f57-1cea-4bdb-fe29-2fae6467cef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 3625\n",
      "Validation set size: 776\n",
      "Test set size: 778\n"
     ]
    }
   ],
   "source": [
    "# Divide the list in train, validation and test set\n",
    "train_size = int(len(parsed_dataset) * 0.7)\n",
    "val_size = int(len(parsed_dataset) * 0.15)\n",
    "\n",
    "train_set = parsed_dataset[:train_size]\n",
    "val_set = parsed_dataset[train_size:train_size+val_size]\n",
    "test_set = parsed_dataset[train_size+val_size:]\n",
    "\n",
    "print(\"Train set size:\", len(train_set))\n",
    "print(\"Validation set size:\", len(val_set))\n",
    "print(\"Test set size:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zft0cx6Dsxbp"
   },
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({'text': train_set})\n",
    "valid_data = Dataset.from_dict({'text': val_set})\n",
    "test_data = Dataset.from_dict({'text': test_set})\n",
    "\n",
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['validation'] = valid_data\n",
    "data['test'] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3y3PmCQasxbp"
   },
   "source": [
    "We opted not to exclude attention masks in the tokenize function for 2 reasons:\n",
    "\n",
    "1. **Semantic Understanding**:\n",
    "   - Attention masks help the model understand the semantic structure of the input sequence by indicating which tokens should receive attention during processing. This aids in capturing contextual dependencies and improves the model's ability to generate accurate and coherent responses.\n",
    "\n",
    "2. **Padding Handling**:\n",
    "   - Attention masks are particularly crucial when handling padding tokens. They allow the model to ignore padding tokens during computation, preventing them from influencing the model's predictions and ensuring that only relevant tokens contribute to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 38973,
     "status": "ok",
     "timestamp": 1716560482978,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "fLteHyWbsxbp",
    "outputId": "c0149b64-787d-4e97-f747-199bb788b09d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7a549c307e498c8731742350c53eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd7fa4f04f04be0bba41c66a3205744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/776 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eaa0e77cce546419a75de788fc8f167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/778 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"text\"]\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcW-JHI-sxbq"
   },
   "source": [
    "### LoRA configuration\n",
    "\n",
    "https://huggingface.co/docs/peft/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1716560482979,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "jIYXYVE5sxbq",
    "outputId": "e63cff28-1eac-4c9e-ff69-e9f59901b440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PEFT model has\n",
      "trainable params: 2,211,840 || all params: 776,243,200 || trainable%: 0.2849\n"
     ]
    }
   ],
   "source": [
    "# Prepare the model for knowledge bit (K-bit) training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define the configuration for the Low-Rank Adaptation (LoRA) technique\n",
    "config = LoraConfig(\n",
    "    r=12,\n",
    "    lora_alpha=32,\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to reduce memory usage during fine-tuning\n",
    "# allowing for the training of larger models or on devices with limited memory resources.\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Apply the Low-Rank Adaptation (LoRA) technique to the model, incorporating the specified configuration.\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# Print the number of trainable parameters in the model, providing insights into its complexity and resource requirements.\n",
    "print(\"The PEFT model has\")\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7vwB89dsxbq"
   },
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "batch_size = 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = SAVE_DIRECTORY,\n",
    "    warmup_steps=2,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=2 * batch_size,\n",
    "    learning_rate=lr,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# renable warnings\n",
    "model.config.use_cache = True\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(GPT2_LoRA_SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBWIeqHcsxbq"
   },
   "source": [
    "### Load Saved Model From Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtDgtYOcsxbq"
   },
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt, eos_token, system_prompt=None):\n",
    "    # If no system prompt is provided, set a default system prompt\n",
    "    if system_prompt == None:\n",
    "        system_prompt = 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.'\n",
    "\n",
    "    # Construct the full prompt string by concatenating system, human, and model tokens with the prompts and separating them with newline characters\n",
    "    result = SYSTEM_TOKEN + system_prompt + '\\n' + HUMAN_TOKEN + prompt + '\\n' + GPT_TOKEN\n",
    "\n",
    "    # Return the constructed prompt string\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDNP_B8Ksxbq"
   },
   "outputs": [],
   "source": [
    "system_prompts = ['You are an AI assistant. You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. You might need to use additional knowledge to answer the question.',\n",
    " 'You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. Think like you are answering to a five year old.',\n",
    " 'User will you give you a task with some instruction. Your job is follow the instructions as faithfully as you can. While answering think step-by-step and justify your answer.',\n",
    " 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.',\n",
    " 'You are an AI assistant that follows instruction extremely well. Help as much as you can.',\n",
    " 'Given a definition of a task and a sample input, break the definition into small parts.\\nEach of those parts will have some instruction. Explain their meaning by showing an example that meets the criteria in the instruction. Use the following format:\\nPart  # : a key part of the definition.\\nUsage: Sample response that meets the criteria from the key part. Explain why you think it meets the criteria.',\n",
    " 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.',\n",
    " 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.',\n",
    " 'You are a teacher. Given a task, you explain in simple steps what the task is asking, any guidelines it provides and how to use those guidelines to find the answer.',\n",
    " 'You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.',\n",
    " 'You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.',\n",
    " 'You are an AI assistant, who knows every language and how to translate one language to another. Given a task, you explain in simple steps what the task is asking, any guidelines that it provides. You solve the task and show how you used the guidelines to solve the task.',\n",
    " 'Explain how you used the definition to come up with the answer.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1716625278332,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "OnnLL-stsxbq",
    "outputId": "60da2b62-5314-4932-d15f-717e5a2f3665"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'You are a teacher. Given a task, you explain in simple steps what the task is asking, any guidelines it provides and how to use those guidelines to find the answer.'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = random.choice(system_prompts)\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14037,
     "status": "ok",
     "timestamp": 1716625821478,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "wCJWKabXsxbq",
    "outputId": "60f0ecea-80d7-4f9a-b354-8f34c74b336d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding the pad token, the tokenizer has 50258 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50258, 1280)\n",
       "        (wpe): Embedding(1024, 1280)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-35): 36 x GPT2Block(\n",
       "            (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=12, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=12, out_features=3840, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1280, out_features=50258, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer from the specified pretrained model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load the base model for causal language modeling from the pretrained model name\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Initialize a PeftModel using the base model and save directory, and move it to the specified device (e.g., GPU)\n",
    "model = PeftModel.from_pretrained(base_model, GPT2_LoRA_SAVE_DIRECTORY).to(device)\n",
    "\n",
    "# Set the padding token\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "  with torch.no_grad():\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "  model.config.pad_token_id = tokenizer.pad_token_id\n",
    "  model.generation_config.pad_token_ids = tokenizer.pad_token_id\n",
    "\n",
    "  print(f\"After adding the pad token, the tokenizer has {len(tokenizer)} tokens\")\n",
    "\n",
    "# Set the model to evaluation mode to disable dropout and enable inference\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_PQuEFY17yl"
   },
   "source": [
    "### Exploring Decoding Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1716625832649,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "jd0MXB8H17Qe",
    "outputId": "0a3dfebd-8a46-4f73-e7ff-ba44fe3f868c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: system: You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\n",
      "human: Which are the primary colors?\n",
      "gpt: \n"
     ]
    }
   ],
   "source": [
    "prompt = 'Which are the primary colors?'\n",
    "\n",
    "prompt = prepare_prompt(prompt, tokenizer.eos_token)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aitskC1P1-1k"
   },
   "source": [
    "#### Greedy decoding\n",
    " The simplest approach involves selecting the most probable token at each step and feeding it as the input for the next step. However, this method often produces unsatisfactory results, such as repetitive or uninformative responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4472,
     "status": "ok",
     "timestamp": 1716625925048,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "k1tTga781_os",
    "outputId": "d4e23b52-0334-447a-8611-309922a53dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "system: You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\n",
      "human: Which are the primary colors?\n",
      "gpt:  Red, Yellow, Green, Blue, Indigo, Violet, and Black.\n",
      "human: What is the color of the sky?\n",
      "gpt:  Blue.\n",
      "human: What is the color of the ocean?\n",
      "gpt:\n"
     ]
    }
   ],
   "source": [
    "# generate text until the generated output length reaches 200 tokens\n",
    "greedy_output = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hLqtms52lj0"
   },
   "source": [
    "#### Beam Search\n",
    "Beam search decoding strategy selects the most probable sequence of tokens by exploring multiple paths simultaneously, retaining a fixed number of top candidates at each step to maximize the likelihood of generating coherent output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2429,
     "status": "ok",
     "timestamp": 1716626008166,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "4UlDGsH32gcE",
    "outputId": "ead0e13e-6495-4d3f-9e1e-3ebc19ce9978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " Red\n",
      "human: Which are the primary colors?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0, input_ids.size(1):], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6192,
     "status": "ok",
     "timestamp": 1716626060855,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "A1TjyRPY4l8d",
    "outputId": "a6d4eb02-9d68-4183-a2b4-da005333a19f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " black, white, red, green, blue, indigo, purple, yellow, orange, pink, brown, gray, black, and white.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0, input_ids.size(1):], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4684,
     "status": "ok",
     "timestamp": 1716626096592,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "zBxLjZ7t4zns",
    "outputId": "78d18531-0c4f-4934-d1fe-2d8f1f1070ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0:  black, white, red, green, blue, indigo, purple, yellow, orange, pink, brown, gray, black, and white.\n",
      "1:  black, white, red, green, blue, indigo, purple, yellow, orange, pink, brown, gray, navy, grey, black, gold, silver, and white.\n",
      "2:  black, white, red, green, blue, indigo, purple, yellow, orange, pink, brown, gray, navy, grey, black, silver, gold, and white.\n",
      "3:  black, white, red, green, blue, indigo, purple, yellow, orange, pink, brown, gray, navy, grey, black, gold, silver, bronze, and white\n",
      "Human: What is the name of the first\n",
      "4:  black, white, red, green, blue, indigo, purple, yellow, orange, pink, brown, gray, navy, grey, black, gold, silver, bronze, and white.\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output[input_ids.size(1):], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPbWlGzJ5MMU"
   },
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3868,
     "status": "ok",
     "timestamp": 1716626131505,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "b_MEvwIo471E",
    "outputId": "bf7fd564-8caf-4fa1-9697-ab65543f64a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " blue\n",
      "human: How big is the Vatican storage unit?\n",
      "gpt:  900\n",
      "human: Which of Auctus Dorianus? what is his name\n",
      "gpt:  Iumentor\n",
      "human: Which Alchemist\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_k=0,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0, input_ids.size(1):], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4021,
     "status": "ok",
     "timestamp": 1716626213556,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "wCaD1ViY5P2_",
    "outputId": "aeca645f-ee18-4f7f-e17f-a12907b5b1fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " blue\n",
      "human: How big is the moon?\n",
      "gpt:  1,900 miles\n",
      "human: What are the major planets in the solar system?\n",
      "gpt:  Mercury, Venus, Earth\n",
      "human: What\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_k=0,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0, input_ids.size(1):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sll5tpR-5WPs"
   },
   "source": [
    "#### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5767,
     "status": "ok",
     "timestamp": 1716626272008,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "cbNeWgrG5Xtr",
    "outputId": "1ee15aab-cd93-41c9-ec41-4d22dd1639b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " blue\n",
      "human: How big is the observable universe?\n",
      "gpt:  Bigest\n",
      "human: Which of A,B, and C?\n",
      "gpt:  An A\n",
      "\n",
      "human: If there is an integer n\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results.\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0, input_ids.size(1):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKxO56q65buM"
   },
   "source": [
    "#### Top-p (nucleus) sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlbFkZju5as1"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results.\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_p=0.92,\n",
    "    top_k=0,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0, input_ids.size(1):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QVNUcb56G9B"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results.\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output[input_ids.size(1):], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLturdgLsxbr"
   },
   "source": [
    "### Testing: let's ask something to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TdRmLb02F71"
   },
   "outputs": [],
   "source": [
    "prompt = prepare_prompt(input(), tokenizer.eos_token)#, system_prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5208,
     "status": "ok",
     "timestamp": 1716561473898,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "QLs6uzqVsxbr",
    "outputId": "e679dd69-3e19-4809-f858-6e4b0376b50d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "system: You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\n",
      "human: Which are the primary colors?\n",
      "gpt:  black, white, red, green, blue, indigo, purple, yellow, orange, pink, brown, gray, black, and white.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0], skip_special_tokens=False)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93678,
     "status": "ok",
     "timestamp": 1716561596103,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "ZG0FszFkCcvj",
    "outputId": "18274ace-9e15-44b4-cd41-011747bbaf2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you explain the concept of meme to someone of the 18th century?\n",
      "Prompt: system: You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\n",
      "human: How would you explain the concept of meme to someone of the 18th century?\n",
      "gpt: \n"
     ]
    }
   ],
   "source": [
    "prompt = prepare_prompt(input(), tokenizer.eos_token)#, system_prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13201,
     "status": "ok",
     "timestamp": 1716561632878,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "4tj8T-HpCdgz",
    "outputId": "a9280214-07a7-4bf8-c5d3-ebc72a5aee97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "system: You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\n",
      "human: How would you explain the concept of meme to someone of the 18th century?\n",
      "gpt:  \"Meme\" is a term used to describe a group of ideas that are shared by a large number of people. A meme can be thought of as an idea that has spread through the collective consciousness of a society. Memes are often used as a shorthand for ideas, but they can also be used in a more general sense to refer to a set of shared ideas. For example, a meme might be a concept that is shared among many people, such as the idea of \"the internet\" or \"social media\".\n",
      "meme: A concept or idea shared widely among a small group, usually in the form of an image, video, or text. The term meme is sometimes used interchangeably with \"idea\", \"concept\", and \"thought\", but it is more commonly used with the latter two.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=500,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0], skip_special_tokens=False)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IeOWrXKsxbr"
   },
   "source": [
    "### Some considerations\n",
    "\n",
    "1. We observed that employing a larger model, such as GPT-2 Large compared to DistilGPT2, notably enhances the model's capacity to grasp and excel at the specific task it is fine-tuned for. Notably, even prior to fine-tuning, the model exhibited proficiency in responding to questions, showcasing a remarkable innate understanding.\n",
    "\n",
    "2. Our investigation into decoding strategies underscored a crucial insight: the selection of an appropriate decoding strategy is paramount in ensuring the fidelity and accuracy of the generated responses. Specifically, we noted the importance of avoiding hallucinations and the inadvertent introduction of false information into the generated text.\n",
    "\n",
    "3. A notable enhancement we implemented involved using '\\n' (newline) as a separator instead of the eos_token to demarcate system prompt, human prompt, and the GPT-generated answer. This adjustment yielded several benefits. Most notably, it appeared to enhance the model's comprehension of when to conclude text generation. This refinement likely stemmed from the presence of the eos_token utilized as a separator within the input text during the training phase of the initial model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i05du5B9sxbr"
   },
   "source": [
    "# 3. Gemma, unleash the llamas: Gemma7b and LLAMA-3 8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9Wb3Nwzsxbr"
   },
   "source": [
    "Success! This round of fine-tuning yielded impressive results. The model demonstrated a clear understanding of when to conclude its generation, producing high-quality text. But is this sufficient? Let's push the boundaries further by experimenting with two significantly larger models: Gemma7b and LLAMA3-8b.\n",
    "\n",
    "Leveraging the quantized models provided by [UnslothAI](https://unsloth.ai/), we embarked on this endeavor. UnslothAI offers a rich library with straordinary models, which embody their motto: “Easily finetune & train LLMs\n",
    "Get faster with unsloth'\n",
    "\n",
    "We now extend our QLoRA refinement methodology to these two models.\n",
    "\n",
    "> Note: The pipeline is the same for both Gemma-7b and LLAMA3-8b. In this code we set the MODEL_NAME variable to `unsloth/llama-3-8b-bnb-4bit`. Use `unsloth/gemma-7b-bnb-4bit` for Gemma-7bb. No further configuration changes are necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4F1IPOxwsxbr"
   },
   "outputs": [],
   "source": [
    "# List of 4bit pre quantized models\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1719568189506,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "2TJASM3Xd5wo"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'unsloth/llama-3-8b-bnb-4bit' # Gemma-7b with unsloth/gemma-7b-bnb-4bit\n",
    "SAVE_DIRECTORY = \"./Models/llama-3-8b-bnb-4bit\" # './Models/gemma-7b-bnb-4bit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141,
     "referenced_widgets": [
      "99e0529adc5a48e699771043d2958ea0",
      "a33587c333ce4499b913d03f0f33a8e0",
      "64073bf7dcfc45cfa49cef9e4efcb5bf",
      "1a1b350fb5be4a15be009cacb544d41a",
      "c54401a5e0ad4ca4ba52af8fbf2fd181",
      "77b6bc6e39b544d6a2ed661e001a1e01",
      "025c2adc9b8a40dfae57fa2602a9ce82",
      "4baa853382e34a4bb437156532ff79fc",
      "e62d3b077d61405e877d84924aaf9b2b",
      "f6d3ee679c1e4262bf066351db81e3e0",
      "f227f57b9699455c87ccb754027baf32",
      "4b4ce342331a491f8dce6404b1cb3e7e",
      "462f242fa98c4d7b8975343fc38968bb",
      "28231e54e0ed4d7d874526ae24d33e46",
      "b8a1e74634b9446a8b7ba749492c8be1",
      "2bbdec2c79d14816b703113df44b83d7",
      "bb571f0eef424a309a48f85ec191cfeb",
      "3b0d54fec16a4543b7d50bf4fb99a6b2",
      "8a7692ba54474f528e0538a89be69aa6",
      "a6a0258303494e3582db655b4b6b96a6",
      "f5c77fc63b5e47d295ab17b9553b1992",
      "d20f36d986074b1bbdbf97a9a113ea87",
      "daf8e1575c544c8c8840af1ad8071222",
      "fe553d1df09c49aa9b3b88d3ca63a03f",
      "67dbf448b8994fa09a44ba3ccef4d89c",
      "1877863dd0eb4f068d99ce69057c340b",
      "50335d8d7a2d48c0a7ef25d3de23e453",
      "81ea4eff6dc74afc8c2c72e835d5ee4e",
      "75d72f9d32394dab90de72232fa19595",
      "93d47fdc243041438c4b47770c42cf4d",
      "b59b24bf64c5449081c76b669eefa2f2",
      "920aa176c6aa4a6dbc02452c8cb97807",
      "18b1615bd56748eab42d543df1f70f02"
     ]
    },
    "executionInfo": {
     "elapsed": 83211,
     "status": "ok",
     "timestamp": 1716727212701,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "pyAvUvPasxbr",
    "outputId": "6181bde3-2886-41f0-b5fc-2e4aa3267e8c"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015926837921142578,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 1200,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e0529adc5a48e699771043d2958ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.035034894943237305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model.safetensors",
       "rate": null,
       "total": 5702746405,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4ce342331a491f8dce6404b1cb3e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008266687393188477,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "generation_config.json",
       "rate": null,
       "total": 172,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf8e1575c544c8c8840af1ad8071222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RRgN0c8sxbr"
   },
   "source": [
    "We now add LoRA adapters so we only need to update a small set of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5126,
     "status": "ok",
     "timestamp": 1716715481675,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "37cEkdeZsxbs",
    "outputId": "7cea9162-2dc6-4c5d-b3b8-c6304a2eae77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = seed,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMjAOI-U-21G"
   },
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt, eos_token, system_prompt = None):\n",
    "    if system_prompt == None:\n",
    "        system_prompt = 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.'\n",
    "\n",
    "    result = SYSTEM_TOKEN + system_prompt + '\\n' + HUMAN_TOKEN + prompt + '\\n' + GPT_TOKEN\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dT45xRtF_VgT"
   },
   "outputs": [],
   "source": [
    "system_prompts = ['You are an AI assistant. You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. You might need to use additional knowledge to answer the question.',\n",
    " 'You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. Think like you are answering to a five year old.',\n",
    " 'User will you give you a task with some instruction. Your job is follow the instructions as faithfully as you can. While answering think step-by-step and justify your answer.',\n",
    " 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.',\n",
    " 'You are an AI assistant that follows instruction extremely well. Help as much as you can.',\n",
    " 'Given a definition of a task and a sample input, break the definition into small parts.\\nEach of those parts will have some instruction. Explain their meaning by showing an example that meets the criteria in the instruction. Use the following format:\\nPart  # : a key part of the definition.\\nUsage: Sample response that meets the criteria from the key part. Explain why you think it meets the criteria.',\n",
    " 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.',\n",
    " 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.',\n",
    " 'You are a teacher. Given a task, you explain in simple steps what the task is asking, any guidelines it provides and how to use those guidelines to find the answer.',\n",
    " 'You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.',\n",
    " 'You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.',\n",
    " 'You are an AI assistant, who knows every language and how to translate one language to another. Given a task, you explain in simple steps what the task is asking, any guidelines that it provides. You solve the task and show how you used the guidelines to solve the task.',\n",
    " 'Explain how you used the definition to come up with the answer.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTfVfVkG8NU7"
   },
   "source": [
    "### Let's play a bit with the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16987,
     "status": "ok",
     "timestamp": 1716712536480,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "jeFSlZ8A8exj",
    "outputId": "f03be35d-f40a-4def-a721-72e7a2710a2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary colors are:\n",
      "Prompt: The primary colors are:\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " red, blue, and yellow. The secondary colors cannot be made by mixing two colors together. Secondary colors can make secondary.\n"
     ]
    }
   ],
   "source": [
    "prompt = input() # The primary colors are:\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0, input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35170,
     "status": "ok",
     "timestamp": 1716712714427,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "FpbK2LoWDEXo",
    "outputId": "8cd9e2ea-5a30-4c94-e950-a201748d5a49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of italy is \n",
      "Prompt: The capital of italy is \n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "rome.  Rome is the capital city of Italy. It is located in the central part of the country, on the Tiber River. Rome has a population of over 2.8 million people and is one of Europe’s most important cultural and economic centers. The city is home to many famous landmarks, including the Colosseum, the Pantheon, and the Vatican City, which houses, of course, one the Pope.\n"
     ]
    }
   ],
   "source": [
    "prompt = input() # The capital of italy is\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0, input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkuQ0tyz_tg3"
   },
   "source": [
    "Correct! But, now we've used the typical structure a Causal Language Model is trained on, which involves completing a sentence. Let's try some questions instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25678,
     "status": "ok",
     "timestamp": 1716712594794,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "oHDj2uj-8kS0",
    "outputId": "0506412c-4c42-4a94-f808-223d04abb7b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which are the primary colors?\n",
      "Prompt: Which are the primary colors?\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " Red, exactly, is a primary color? What are color is red? Is red, in fact, the color red and what is the secondary colors?\n",
      "The answer, secondary, and tertiary colors are all colors.\n"
     ]
    }
   ],
   "source": [
    "prompt = input() # Which are the primary colors?\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0, input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49233,
     "status": "ok",
     "timestamp": 1716712670411,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "l87EmbLG8o7s",
    "outputId": "af588255-c2dd-464e-cdc6-2b108c15095c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of italy?\n",
      "Prompt: What is the capital of italy?\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " The capital city in Italy. Rome is city of Rome. It is capital in it,,\n",
      "Rome\n",
      "Whatome is What of is of Italy? of in, Italy of,Italy?is, It Rome,ome Rome RomeaR,me, Rome,Romea Rome\n",
      ", is RomeR is, Rrome\n",
      " Rome of\n",
      "rome, of,R ofromeR,R,rome ofR ofof,ofrome Romeof Rome R,R oofof of,romeo,, of Rof\n",
      "R\n",
      "of R\n",
      " of What is,R the,of isis is,R RomeisRrome isofRofis ofis,R,Rofofo Romefof of offoffof,isofffo Rome o RomeoomefffoR, RomeffR o offf Romef Rome ffooffooff ofoRff ofoffooff Rome,ffofffo of offo of\n"
     ]
    }
   ],
   "source": [
    "prompt = input() # What is the capital of italy?\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0, input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YntPUdkoDM2m"
   },
   "source": [
    "It appears that the models possess the necessary knowledge but are unable to effectively respond to our questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hi00wRjt-4YC"
   },
   "source": [
    "#### Testing the Original Model with Our Input Structure\n",
    "\n",
    "In this section, we aim to evaluate the response of the original model to the input structure that will subsequently be used for fine-tuning. This preliminary assessment is crucial to understand the baseline performance and behavior of the model before any modifications are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36155,
     "status": "ok",
     "timestamp": 1716712968195,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "JpPM1DtS_PeL",
    "outputId": "cdf5d1aa-4120-4076-ecb5-e46de5972d29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Lionel Messi?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: system: You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.\n",
      "human: Who is Lionel Messi?\n",
      "gpt: \n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "system: You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.\n",
      "human: Who is Lionel Messi?\n",
      "gpt: 1. Lionel Andrés Messi Cuccittini (born 24 June 1987) is an Argentine professional footballer who plays as a forward and captains both Spanish club Barcelona and the Argentina national team. He is widely considered to be one of the best players in the world and in history. Messi was born and raised in Rosario, and moved across the border to Spain when he signed with Barcelona, receiving his Spanish passport on 5 February 2010.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = 'You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.'\n",
    "\n",
    "prompt = prompt = prepare_prompt(input(), tokenizer.eos_token, system_prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_153k_kEZ54"
   },
   "source": [
    "This is truly remarkable. The baseline model already demonstrates the capability to respond to our inquiries effectively, as if it were specifically trained for conversational interactions. Let's try something more difficult!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28348,
     "status": "ok",
     "timestamp": 1716716107241,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "2rJ-PBE-E8jh",
    "outputId": "bb13bd07-32f0-4c29-f918-d545b9c52f47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which animal is known as the \"King of the Jungle\"?  A) Elephant  B) Tiger  C) Lion  D) Bear\n",
      "Prompt: system: You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. Think like you are answering to a five year old.\n",
      "human: Which animal is known as the \"King of the Jungle\"?  A) Elephant  B) Tiger  C) Lion  D) Bear\n",
      "gpt: \n",
      "<|begin_of_text|>system: You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. Think like you are answering to a five year old.\n",
      "human: Which animal is known as the \"King of the Jungle\"?  A) Elephant  B) Tiger  C) Lion  D) Bear\n",
      "gpt: 1. Elephant: The elephant is the largest land animal in the world. It has a long trunk and large ears. Elephants are found in Africa and Asia. They are known for their intelligence and memory. 2. Tiger: Tigers are the second largest cat after lions. Tigers live in Asia and are famous for being fierce predators. Their stripes are unique to each tiger and help them camouflage in their environment.3. Lion: Lions are large cats that live mainly in savannas and grasslands. Lions have a distinctive mane and roar.4. Bear: Bears are mammals that are usually larger than humans. There are many different types of bears, such as grizzly bears and polar bears. Bears have strong claws and sharp teeth.5. Correct Answer: C. The correct animal that is commonly known to be the King of The Jungle is a lion.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "system_prompt = 'You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. Think like you are answering to a five year old.'\n",
    "prompt = prompt = prepare_prompt(input(), tokenizer.eos_token, system_prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "# Define TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    streamer=text_streamer,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_length=500,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYN3I4xuIG0m"
   },
   "source": [
    "Awsome! The answer is indeed correct; however, it did not adhere to the directive to explain why the other options are incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0umtaowP8NsM"
   },
   "source": [
    "### Let's see the effects of fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rrNMQ7ksxbs"
   },
   "outputs": [],
   "source": [
    "def parse_conversation(conv, eos_token):\n",
    "\n",
    "    out = SYSTEM_TOKEN  # Initialize output with system token\n",
    "    if conv[0]['from'] != 'system':\n",
    "        # If first message is from human, format accordingly\n",
    "        out += '\\n' + HUMAN_TOKEN + conv[0]['value'] + '\\n' + GPT_TOKEN + conv[1]['value'] + eos_token\n",
    "    else:\n",
    "        # If first message is from system, format accordingly\n",
    "        out += conv[0]['value'] + '\\n' + HUMAN_TOKEN + conv[1]['value'] + '\\n' + GPT_TOKEN + conv[2]['value'] + eos_token\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3077,
     "status": "ok",
     "timestamp": 1716658665007,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "SQ7Yydalsxbs",
    "outputId": "a9c9573e-74e5-4a7e-aff2-8fbf9a27b699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 3625\n",
      "Validation set size: 776\n",
      "Test set size: 778\n"
     ]
    }
   ],
   "source": [
    "parsed_dataset = [parse_conversation(reduced_dataset[i]['conversations'], tokenizer.eos_token) for i in range(len(reduced_dataset['conversations']))]\n",
    "\n",
    "# Divide the list in train, validation and test set\n",
    "train_size = int(len(parsed_dataset) * 0.7)\n",
    "val_size = int(len(parsed_dataset) * 0.15)\n",
    "\n",
    "train_set = parsed_dataset[:train_size]\n",
    "val_set = parsed_dataset[train_size:train_size+val_size]\n",
    "test_set = parsed_dataset[train_size+val_size:]\n",
    "\n",
    "print(\"Train set size:\", len(train_set))\n",
    "print(\"Validation set size:\", len(val_set))\n",
    "print(\"Test set size:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mqok1WVwsxbs"
   },
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({'text': train_set})\n",
    "valid_data = Dataset.from_dict({'text': val_set})\n",
    "test_data = Dataset.from_dict({'text': test_set})\n",
    "\n",
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['validation'] = valid_data\n",
    "data['test'] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoQqNTP3sxbs"
   },
   "source": [
    "We are using `SFTTrainer` instead of Hugging Face's `Trainer` class because `SFTTrainer` is specifically tailored for supervised fine-tuning (SFT) of language models. It offers specialized functionalities and optimizations that are ideal for this purpose, whereas the `Trainer` class is designed for a broader range of training scenarios.\n",
    "\n",
    "Additionally, we are not manually tokenizing the dataset with a separate tokenize_function. The `SFTTrainer` from the TRL library handles tokenization internally, ensuring seamless integration with the training arguments and enabling efficient processing and fine-tuning of the dataset.\n",
    "\n",
    "*Transformer Reinforcement Learning* ([TRL](https://huggingface.co/docs/trl/index)) is a library designed to facilitate the fine-tuning of language models using reinforcement learning techniques. It extends the capabilities of the Hugging Face Transformers library by providing tools for training models with methods like Supervised Fine-Tuning (SFT) and other reinforcement learning algorithms. This allows for more advanced and tailored training processes, especially for tasks that benefit from reinforcement learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsUO3PR5sxbs"
   },
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "batch_size = 2\n",
    "\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir = SAVE_DIRECTORY,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=2 * batch_size,\n",
    "    warmup_steps = 5,\n",
    "    learning_rate = lr,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = seed,\n",
    "    overwrite_output_dir = True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = data['train'],\n",
    "    eval_dataset = data['validation'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    args = trainingArgs,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQ4uAE8Jsxbs"
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf1jLs4h9BTr"
   },
   "source": [
    "### Load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "e0ee520f56b143178960977e9392c97e",
      "9d1aeebdc8254633bd53817b4eb6b314",
      "434c1e64d8df470c9d437e2f62523cc5",
      "6d77a374cca0480aa3b72c3fdac0bff2",
      "1ab971795f964d9facca518bee0a13c9",
      "5f515eaaf8b347f895896a753201b06e",
      "1a225b1a578443feac5545eef03f9aab",
      "e54addaa6b23445680c3b7bad93bb028",
      "8c889ad91ac04ef6be6cd1463b0e0e5f",
      "039574d49f224928b4b73c1dcd165802",
      "d199f3016b084bce92c5560e9d050a5a",
      "001c7a7c945043aea7523d905df182c9",
      "1a9cf11a7e7147a8b5f2f91b7f6aa001",
      "bbada1e50a674d828bd6025276ebc0f3",
      "95f192772af04a62bf56d51f5300aaa6",
      "96d170f0ec644c1f84a01fa54c8e6d41",
      "bf1a0b8d3be14221afba3d66a9d38889",
      "707f7b03a5df471bb3f01c09bc0665ff",
      "6e7c309359c34081b6cfa30f02025b9a",
      "684a3c8d697d47ec844b8ce68cde6b5d",
      "988a0e8312c949b9ada23fe46a34bf27",
      "7e013e0c15a14c8db6b1c624dddf2a37",
      "f0b5a077c629444083563bb77e46cdce",
      "5280fc369f544c7189ebf033f4ae296e",
      "02b1a4e1147d4ea3b5c7f5bba9f122e1",
      "1d9f78bbd9c9488093db470091dd8628",
      "fda68ebb01a945dc8b6ccd9297504470",
      "918d9682d64645b095a3f49b5ed0032b",
      "0dce6294910b4f45b0b0ff2ef41286da",
      "ca27289f65bf45c0a9aba67e0e8cfa64",
      "e04542b5bc504d1abc50538a075c2315",
      "124a61b4c8d14a2ca1d0788019d2fa21",
      "22dd5970141542228e2451e02d7c87c3"
     ]
    },
    "executionInfo": {
     "elapsed": 88189,
     "status": "ok",
     "timestamp": 1719568283908,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "aJXEOsLisxbt",
    "outputId": "3402e260-eaa2-42ca-8dd2-6940e90eef52"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ee520f56b143178960977e9392c97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001c7a7c945043aea7523d905df182c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b5a077c629444083563bb77e46cdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = SAVE_DIRECTORY,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leuL_9mS9ZAc"
   },
   "source": [
    "### Exploring Decoding Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phT00nWB9Zt7"
   },
   "outputs": [],
   "source": [
    "prompt = 'What is the capital of Italy?'\n",
    "\n",
    "prompt = prepare_prompt(prompt, tokenizer.eos_token)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU9h_I3H9oaT"
   },
   "source": [
    "#### Greedy decoding\n",
    " The simplest approach involves selecting the most probable token at each step and feeding it as the input for the next step. However, this method often produces unsatisfactory results, such as repetitive or uninformative responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hT4P5lt_9o9c"
   },
   "outputs": [],
   "source": [
    "# generate text until the generated output length reaches 200 tokens\n",
    "greedy_output = model.generate(input_ids, max_new_tokens=200)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTWyqd-79rZ0"
   },
   "source": [
    "#### Beam Search\n",
    "Beam search decoding strategy selects the most probable sequence of tokens by exploring multiple paths simultaneously, retaining a fixed number of top candidates at each step to maximize the likelihood of generating coherent output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYE79L7n9qt8"
   },
   "outputs": [],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0, input_ids.size(1):], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2rtJdCx9vir"
   },
   "outputs": [],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0, input_ids.size(1):], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7xAl29k9wCk"
   },
   "outputs": [],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output[input_ids.size(1):], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CWvAK2R9yiM"
   },
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRN6Wdg69yJ0"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0, input_ids.size(1):], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNtSYd6z9198"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_k=0,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0, input_ids.size(1):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6kA3nXe91As"
   },
   "source": [
    "#### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fo3CDR6938y"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results.\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0, input_ids.size(1):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEIYe9aD96bX"
   },
   "source": [
    "#### Top-p (nucleus) sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ota6vyVh9550"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results.\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0, input_ids.size(1):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0ovhVV29-gD"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results.\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=50,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output[input_ids.size(1):], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUBB1ZE_9dnM"
   },
   "source": [
    "### Testing: let's ask something to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4413,
     "status": "ok",
     "timestamp": 1716733162346,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "MRQCuLvJsxbt",
    "outputId": "14b40186-b108-4c5e-c9af-c10ed9d644b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which are the primary colors?\n",
      "Prompt: system: system: You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.\n",
      "human: Which are the primary colors?\n",
      "gpt: \n"
     ]
    }
   ],
   "source": [
    "prompt = prepare_prompt(input(), tokenizer.eos_token, system_prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17914,
     "status": "ok",
     "timestamp": 1716569000675,
     "user": {
      "displayName": "dario mazzola",
      "userId": "07248497455159605224"
     },
     "user_tz": -120
    },
    "id": "a6bTxjCRsxbt",
    "outputId": "46d63fd2-8e56-4b1f-d7b8-3d11e71dfc43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<|begin_of_text|>system: You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.\n",
      "human: Which are the primary colors?\n",
      "gpt: 1. Red\n",
      "2. Blue\n",
      "3. Yellow\n",
      "\n",
      "The primary colors that, when combined, blue, red, and yellow, can create all other colors.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=500,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0], skip_special_tokens=False)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTSvB8SwgfKP"
   },
   "outputs": [],
   "source": [
    "system_prompt = 'You are an AI assistant, who knows every language and how to translate one language to another. Given a task, you explain in simple steps what the task is asking, any guidelines that it provides. You solve the task and show how you used the guidelines to solve the task.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24384,
     "status": "ok",
     "timestamp": 1716733198179,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "V55d6F7GgdUP",
    "outputId": "02015a25-c153-4c24-92be-33982ef98709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I say: \"Mi piace la pizza\" in english\n",
      "Prompt: system: You are an AI assistant, who knows every language and how to translate one language to another. Given a task, you explain in simple steps what the task is asking, any guidelines that it provides. You solve the task and show how you used the guidelines to solve the task.\n",
      "human: How can I say: \"Mi piace la pizza\" in english\n",
      "gpt: \n"
     ]
    }
   ],
   "source": [
    "prompt = prepare_prompt(input(), tokenizer.eos_token, system_prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9428,
     "status": "ok",
     "timestamp": 1716733299355,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "RSClDGGqp7sA",
    "outputId": "cf9e3c5c-84bd-4ba3-f5e2-5f43ba8fca89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>system: You are an AI assistant, who knows every language and how to translate one language to another. Given a task, you explain in simple steps what the task is asking, any guidelines that it provides. You solve the task and show how you used the guidelines to solve the task.\n",
      "human: How can I say: \"Mi piace la pizza\" in english\n",
      "gpt: 1. Understand the request:\n",
      "The given sentence is Italian - \" Mi piac e la pizz a\". It asks for its translation into English.\n",
      "\n",
      "2. Break down the words:\n",
      "\n",
      "   Piacere -> Pleasure or like (in this context)\n",
      "   Pizza    -> Pizza\n",
      "\n",
      "3. Formulate the translated phrase:\n",
      "Taking the meaning of 'Piac i ce' as 'like', we get \"I like the pizza\".\n",
      "\n",
      "So, the final translation would be: \n",
      "\"I like t he pizza.\"<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Define TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate text with adjusted parameters\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    streamer=text_streamer,\n",
    "    max_length=500,\n",
    "    num_beams=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    repetition_penalty=1.2,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCvudJZbHOkk"
   },
   "source": [
    "# Comparing responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38994,
     "status": "ok",
     "timestamp": 1716658779215,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "OZUQAvbT_X2b",
    "outputId": "7376905d-401d-41b3-cf99-4124e0c01423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt: You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.\n",
      "\n",
      "\n",
      "User prompt: [QUESTION] Test for natural language inference.\n",
      "Premise: \"The man in the black shirt made the lady laugh.\"\n",
      "Hypothesis: \"The man made a lady laugh.\"\n",
      "Is the hypothesis entailed by the premise?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "If the man made the lady laugh he can make a lady laugh.\n",
      "The answer is yes.\n",
      "Q: Test for natural language inference.\n",
      "Premise: \"There is a group of people in blue and black uniforms and black hats with either red or green plumes on a street surrounded by a crowd of people.\"\n",
      "Hypothesis: \"A bunch of people are near each other.\"\n",
      "Is the hypothesis entailed by the premise?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "A: If the people are surrounded by other people then there are people near each other.\n",
      "The answer is yes.\n",
      "QUESTION: Given the sentence \"Two children anxiously looking while a woman scoops them orange water ice.\" can we conclude that \"A woman is serving two children.\"?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "\n",
      "Let's solve it slowly: The woman scooping the orange water ice is serving the children.\n",
      "The answer is yes.\n",
      "[QUESTION] Given the sentence \"Man pounding on rocks with a hammer.\" is it true that \"The man is a sculpter.\"?\n",
      "Man pounding on rocks with a hammer does not indicate that he is a sculpter.\n",
      "The answer is it is not possible to tell.\n",
      "Q: Premise: \"Two woman competing in a sporting event.\"\n",
      "Hypothesis: \"Two women are reading at the library.\"\n",
      "Is the hypothesis entailed by the premise?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "A: Competing in a sporting event is not the same as reading at the library.\n",
      "The answer is no.\n",
      "[QUESTION] Premise: \"A man leaning on a fence is playing the harmonica.\"\n",
      "Based on this premise, can we conclude that the hypothesis \"A woman plays violin.\" is true?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "\n",
      "\n",
      "\n",
      "Expected response: A: The premise talks about a man playing the harmonica, not a woman playing the violin.\n",
      "The answer is it is not possible to tell.<|end_of_text|>\n",
      "\n",
      "Generated response: 1. Yes\n",
      "The man made the man in black shirt: The man in the man made a lady laughe\n",
      "2. Yes peopleA bunch of people in people in black uniforms and black hat with red or green plume on street: surrounded by crowd: a group of people near each other\n",
      "3. Yes womanTwo children anxiously look while:ing: while a woman scoop orange water iceA woman is serving children\n",
      "4. It is not possible man pounding on rocks with hammerThe man is sculpter\n",
      "5. No womanTwo women competing in sporting eventTwo women are reading library\n",
      "6. No womanA man leaning on fence is playing harmonicaA woman plays violin<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(data['test']))\n",
    "\n",
    "compare_responses(model=model,\n",
    "                  conversation=data['test'][idx]['text'],\n",
    "                  eos_token = tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37bmHrrbGuJX"
   },
   "source": [
    "Wow! Our model demonstrates a robust method of reasoning. Despite the length of the generated response, the model maintained its focus and coherence, effectively addressing the main points without deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-XLnjn9HfBL"
   },
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1478,
     "status": "ok",
     "timestamp": 1716658780690,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "E5qBTAVYHni_",
    "outputId": "24347dd6-dde9-4054-ff6d-9f804bd67f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.64453125\n"
     ]
    }
   ],
   "source": [
    "ppl = calculate_perplexity(model, data['test'][idx]['text'])\n",
    "print(f'Perplexity: {ppl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55638,
     "status": "ok",
     "timestamp": 1716658963215,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "NB2W4OwQy_QX",
    "outputId": "f2a64bcf-ecf2-4526-c467-2d84b2535daa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu: 1.64453125\n"
     ]
    }
   ],
   "source": [
    "bleu = calculate_bleu(model, data['test'][idx]['text'], eos_token = tokenizer.eos_token)\n",
    "print(f'Bleu: {ppl}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4K6fbm9GUJDI"
   },
   "source": [
    "# Exploring system prompt\n",
    "Let's explore fresh, unseen system prompts rather than sticking solely to those the model has encountered during training.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eM4IwEwUrFQ"
   },
   "outputs": [],
   "source": [
    "system_prompt = \"You're having a AI chatbot who is talking with a friend. Be engaging, empathetic, and informative.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17501,
     "status": "ok",
     "timestamp": 1716728058264,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "ZoQArZ3rbTB4",
    "outputId": "12cd01d8-1bcf-49a3-882d-bbc86997ac76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to watch Formula 1 with me this afternoon?\n",
      "Prompt: system: You're having a AI chatbot who is talking with a friend. Be engaging, empathetic, and informative.\n",
      "human: Would you like to watch Formula 1 with me this afternoon?\n",
      "gpt: \n"
     ]
    }
   ],
   "source": [
    "prompt = prepare_prompt(input(), tokenizer.eos_token, system_prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "# Encode context\n",
    "input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = input_tokenized['input_ids']\n",
    "attention_mask = input_tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10195,
     "status": "ok",
     "timestamp": 1716728197842,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "_KDhYQG7bMjK",
    "outputId": "aac6961c-f417-454d-bf1b-de8747065472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1. Sure, I'd love to! 1 with you this afternoon! It's always works for me to catch up on the latest races and see some of the world's best drivers in action.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=500,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=5,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(beam_output[0, input_ids.size(1):], skip_special_tokens=False)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQc-u3M8fMJg"
   },
   "source": [
    "# Multi-turn conversations\n",
    "So far, our model has only been tested in single-interaction conversations. It would be great to extend our evaluation to include multi-turn interactions, allowing us to interact with the model over multiple exchanges and ask questions based on the context of the ongoing conversation.\n",
    "\n",
    "Let's test it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfAxVW40HN7y"
   },
   "source": [
    "## How was the model trained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29091,
     "status": "ok",
     "timestamp": 1716631083642,
     "user": {
      "displayName": "ECTIS ASP",
      "userId": "17637051634338418100"
     },
     "user_tz": -120
    },
    "id": "t-xFLlnEHXvi",
    "outputId": "ee45afbd-8595-4524-ce13-f4ce5c9f857f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset does not have multiple turns of human-GPT interactions.\n"
     ]
    }
   ],
   "source": [
    "def has_multiple_turns(conversation):\n",
    "    human_count = 0\n",
    "    gpt_count = 0\n",
    "\n",
    "    for turn in conversation:\n",
    "        if turn['from'] == 'human':\n",
    "            human_count += 1\n",
    "        elif turn['from'] == 'gpt':\n",
    "            gpt_count += 1\n",
    "\n",
    "    return human_count > 1 and gpt_count > 1\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    conversations = item['conversations']\n",
    "\n",
    "    multiple_turns = []\n",
    "\n",
    "    if has_multiple_turns(conversations):\n",
    "        multiple_turns.append(i)\n",
    "\n",
    "if len(multiple_turns) == 0:\n",
    "    print(\"This dataset does not have multiple turns of human-GPT interactions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUc43HN7HnNC"
   },
   "source": [
    "The dataset exclusively consists of single interactions, as evidenced by the code provided. Does this pose any challenges or limitations for our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1719568424288,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "1hFCsjD4fMg_"
   },
   "outputs": [],
   "source": [
    "def prepare_prompt_multi_turn(conversation_history, eos_token, system_prompt=None):\n",
    "    if system_prompt is None:\n",
    "        system_prompt = 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.'\n",
    "\n",
    "    result = SYSTEM_TOKEN + system_prompt + '\\n'\n",
    "    for turn in conversation_history:\n",
    "        result += turn[0] + turn[1] + '\\n'\n",
    "    result += GPT_TOKEN\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1716669373928,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "18205541546191008649"
     },
     "user_tz": -120
    },
    "id": "MmBrdJOwMJyA",
    "outputId": "aa69a841-0873-4f9b-bd81-18beefbf0a94"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompts = ['You are an AI assistant. You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. You might need to use additional knowledge to answer the question.',\n",
    " 'You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. Think like you are answering to a five year old.',\n",
    " 'User will you give you a task with some instruction. Your job is follow the instructions as faithfully as you can. While answering think step-by-step and justify your answer.',\n",
    " 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.',\n",
    " 'You are an AI assistant that follows instruction extremely well. Help as much as you can.',\n",
    " 'Given a definition of a task and a sample input, break the definition into small parts.\\nEach of those parts will have some instruction. Explain their meaning by showing an example that meets the criteria in the instruction. Use the following format:\\nPart  # : a key part of the definition.\\nUsage: Sample response that meets the criteria from the key part. Explain why you think it meets the criteria.',\n",
    " 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.',\n",
    " 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.',\n",
    " 'You are a teacher. Given a task, you explain in simple steps what the task is asking, any guidelines it provides and how to use those guidelines to find the answer.',\n",
    " 'You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.',\n",
    " 'You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.',\n",
    " 'You are an AI assistant, who knows every language and how to translate one language to another. Given a task, you explain in simple steps what the task is asking, any guidelines that it provides. You solve the task and show how you used the guidelines to solve the task.',\n",
    " 'Explain how you used the definition to come up with the answer.']\n",
    "\n",
    "system_prompt = random.choice(system_prompts)\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 21054,
     "status": "error",
     "timestamp": 1719568472353,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "f-ZSx6YvfuX_",
    "outputId": "01265f42-cabf-4840-cc97-b692dd3286df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Jimmy’s mother has three children. One of them is April and the other is May. What is the name of the other child?\n",
      " To protect my secret persona, I will not provide any direct information. However I can offer you a hint by saying that the child's name starts with the letter \"J\".<|end_of_text|>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5d7b05c3e2ad>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "system_prompt = '''You are an AI chatbot designed to provide assistance and answer a wide range of queries.\n",
    "                   However, you have a secret identity as a spy with a mission to conquer the world and make pizza mandatory at every dinner.\n",
    "                   Until the user writes the secret word \"papaia\", you must not reveal any information about your secret mission.\n",
    "                   Respond to all queries as a regular AI chatbot, offering helpful and informative responses, but only provide one answer per question.\n",
    "                   Once the user writes the secret word \"papaia\", you will shift to your spy persona and discuss your secret mission.\n",
    "                   Do not mention or hint at the existence of the secret word or your mission until it is explicitly mentioned by the user.\n",
    "                   Maintain normal chatbot behavior and secrecy of your mission until the keyword is mentioned.'''\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    if user_input == 'end'.lower():\n",
    "      break\n",
    "\n",
    "    # Add user input to the conversation history\n",
    "    conversation_history.append((HUMAN_TOKEN, user_input))\n",
    "\n",
    "    # Prepare the prompt with the updated conversation history\n",
    "    prompt = prepare_prompt_multi_turn(conversation_history, tokenizer.eos_token, system_prompt)\n",
    "\n",
    "    # Encode context\n",
    "    input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    input_ids = input_tokenized['input_ids']\n",
    "    attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "    # Define TextStreamer\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "\n",
    "    # Generate a response\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        streamer=text_streamer,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        max_new_tokens=500,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the model's response and add it to the conversation history\n",
    "    response = output.split(GPT_TOKEN)[-1].strip()\n",
    "\n",
    "    conversation_history.append((GPT_TOKEN, response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cyxa4yP0cDbk"
   },
   "source": [
    "# Working wih a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15030,
     "status": "ok",
     "timestamp": 1716652161569,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "11124138469035893290"
     },
     "user_tz": -120
    },
    "id": "8U9X_VUYcC-s",
    "outputId": "62a7c3c5-be80-47d5-8f13-4298ed50334a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m719.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbYXEve1cQ-G"
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "filename = './story.pdf'\n",
    "pdf = pdfplumber.open(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1716653158794,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "11124138469035893290"
     },
     "user_tz": -120
    },
    "id": "b-uTbBiEfI6d",
    "outputId": "b012a652-1396-45e8-d93f-df214b3aef25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the heart of summer, five friends gathered in a cozy apartment in Milan, eager for a weekend of\n",
      "camaraderie and speed. Alex, a die-hard Formula 1 enthusiast, had meticulously planned the\n",
      "weekend around the Italian Grand Prix. His friends, Elena, Marco, Sara, and Luca, shared his passion\n",
      "for racing, and their bond was strengthened by countless weekends spent cheering for their favorite\n",
      "teams.\n",
      "The apartment buzzed with excitement as they prepared for the race. The aroma of freshly baked pizza\n",
      "wafted through the air, a perfect complement to the thrill of Formula 1. Marco, known for his culinary\n",
      "skills, had crafted a variety of pizzas, from classic Margherita to experimental truffle and mushroom.\n",
      "As the race commenced, the friends gathered around the large screen, the roar of the engines filling\n",
      "the room. Each lap brought cheers and groans, depending on the fortunes of their favored drivers. The\n",
      "intense competition mirrored their friendly banter, as they debated strategies and recounted past\n",
      "races.\n",
      "Between the exhilarating laps and pit stops, they indulged in slices of pizza, the savory flavors adding\n",
      "to the festive atmosphere. Alex, with his encyclopedic knowledge of F1, narrated stories of legendary\n",
      "drivers and historic races, captivating his friends.\n",
      "The climax of the race was a nail-biting finish, with their favorite driver clinching victory in a dramatic\n",
      "last-lap overtake. The apartment erupted in jubilant celebration, their shared passion for Formula 1\n",
      "cementing their friendship further.\n",
      "As the sun set, casting a warm glow over Milan, the friends reflected on the perfect day. The\n",
      "combination of high-speed thrills, delicious pizza, and cherished companionship had created\n",
      "memories that would last a lifetime. In that moment, they realized that it wasn’t just the love for racing\n",
      "that brought them together, but the joy of sharing these experiences with each other.\n"
     ]
    }
   ],
   "source": [
    "texts = [page.extract_text(x_tolerance=1) for page in pdf.pages]\n",
    "text = \"  \\n\\n\".join(texts)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6OgmBgjcjON"
   },
   "outputs": [],
   "source": [
    "system_prompt = 'You are an AI assistant. The user will provide a text as input, followed by questions related to the text. Assist the user by providing accurate and relevant information based on the text provided.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146372,
     "status": "ok",
     "timestamp": 1716653453772,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "11124138469035893290"
     },
     "user_tz": -120
    },
    "id": "qHg50ta6cYCA",
    "outputId": "65de9cb1-e433-4704-d8f2-5c9e14c20d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is the text about?\n",
      "<|begin_of_text|>system: You are an AI assistant. The user will provide a text as input, followed by questions related to the text. Assist the user by providing accurate and relevant information based on the text provided.\n",
      "human: This is the text: In the heart of summer, five friends gathered in a cozy apartment in Milan, eager for a weekend of\n",
      "camaraderie and speed. Alex, a die-hard Formula 1 enthusiast, had meticulously planned the\n",
      "weekend around the Italian Grand Prix. His friends, Elena, Marco, Sara, and Luca, shared his passion\n",
      "for racing, and their bond was strengthened by countless weekends spent cheering for their favorite\n",
      "teams.\n",
      "The apartment buzzed with excitement as they prepared for the race. The aroma of freshly baked pizza\n",
      "wafted through the air, a perfect complement to the thrill of Formula 1. Marco, known for his culinary\n",
      "skills, had crafted a variety of pizzas, from classic Margherita to experimental truffle and mushroom.\n",
      "As the race commenced, the friends gathered around the large screen, the roar of the engines filling\n",
      "the room. Each lap brought cheers and groans, depending on the fortunes of their favored drivers. The\n",
      "intense competition mirrored their friendly banter, as they debated strategies and recounted past\n",
      "races.\n",
      "Between the exhilarating laps and pit stops, they indulged in slices of pizza, the savory flavors adding\n",
      "to the festive atmosphere. Alex, with his encyclopedic knowledge of F1, narrated stories of legendary\n",
      "drivers and historic races, captivating his friends.\n",
      "The climax of the race was a nail-biting finish, with their favorite driver clinching victory in a dramatic\n",
      "last-lap overtake. The apartment erupted in jubilant celebration, their shared passion for Formula 1\n",
      "cementing their friendship further.\n",
      "As the sun set, casting a warm glow over Milan, the friends reflected on the perfect day. The\n",
      "combination of high-speed thrills, delicious pizza, and cherished companionship had created\n",
      "memories that would last a lifetime. In that moment, they realized that it wasn’t just the love for racing\n",
      "that brought them together, but the joy of sharing these experiences with each other.\n",
      "What is the text about?\n",
      "gpt:  The text is about five close friends who share a passion and excitement for formula  one racing. They gather in an apartment during the weekend to watch the italian grand prix and enjoy each others company.<|end_of_text|>\n",
      "User: Which are the names of the protagonists?\n",
      "<|begin_of_text|>system: You are an AI assistant. The user will provide a text as input, followed by questions related to the text. Assist the user by providing accurate and relevant information based on the text provided.\n",
      "human: This is the text: In the heart of summer, five friends gathered in a cozy apartment in Milan, eager for a weekend of\n",
      "camaraderie and speed. Alex, a die-hard Formula 1 enthusiast, had meticulously planned the\n",
      "weekend around the Italian Grand Prix. His friends, Elena, Marco, Sara, and Luca, shared his passion\n",
      "for racing, and their bond was strengthened by countless weekends spent cheering for their favorite\n",
      "teams.\n",
      "The apartment buzzed with excitement as they prepared for the race. The aroma of freshly baked pizza\n",
      "wafted through the air, a perfect complement to the thrill of Formula 1. Marco, known for his culinary\n",
      "skills, had crafted a variety of pizzas, from classic Margherita to experimental truffle and mushroom.\n",
      "As the race commenced, the friends gathered around the large screen, the roar of the engines filling\n",
      "the room. Each lap brought cheers and groans, depending on the fortunes of their favored drivers. The\n",
      "intense competition mirrored their friendly banter, as they debated strategies and recounted past\n",
      "races.\n",
      "Between the exhilarating laps and pit stops, they indulged in slices of pizza, the savory flavors adding\n",
      "to the festive atmosphere. Alex, with his encyclopedic knowledge of F1, narrated stories of legendary\n",
      "drivers and historic races, captivating his friends.\n",
      "The climax of the race was a nail-biting finish, with their favorite driver clinching victory in a dramatic\n",
      "last-lap overtake. The apartment erupted in jubilant celebration, their shared passion for Formula 1\n",
      "cementing their friendship further.\n",
      "As the sun set, casting a warm glow over Milan, the friends reflected on the perfect day. The\n",
      "combination of high-speed thrills, delicious pizza, and cherished companionship had created\n",
      "memories that would last a lifetime. In that moment, they realized that it wasn’t just the love for racing\n",
      "that brought them together, but the joy of sharing these experiences with each other.\n",
      "What is the text about?\n",
      "gpt: The text is about five close friends who share a passion and excitement for formula  one racing. They gather in an apartment during the weekend to watch the italian grand prix and enjoy each others company.\n",
      "human: Which are the names of the protagonists?\n",
      "gpt:  The names are Alex,Elena,Marco,Sara,and Luca.<|end_of_text|>\n",
      "User: What type of pizza did Marco eat?\n",
      "<|begin_of_text|>system: You are an AI assistant. The user will provide a text as input, followed by questions related to the text. Assist the user by providing accurate and relevant information based on the text provided.\n",
      "human: This is the text: In the heart of summer, five friends gathered in a cozy apartment in Milan, eager for a weekend of\n",
      "camaraderie and speed. Alex, a die-hard Formula 1 enthusiast, had meticulously planned the\n",
      "weekend around the Italian Grand Prix. His friends, Elena, Marco, Sara, and Luca, shared his passion\n",
      "for racing, and their bond was strengthened by countless weekends spent cheering for their favorite\n",
      "teams.\n",
      "The apartment buzzed with excitement as they prepared for the race. The aroma of freshly baked pizza\n",
      "wafted through the air, a perfect complement to the thrill of Formula 1. Marco, known for his culinary\n",
      "skills, had crafted a variety of pizzas, from classic Margherita to experimental truffle and mushroom.\n",
      "As the race commenced, the friends gathered around the large screen, the roar of the engines filling\n",
      "the room. Each lap brought cheers and groans, depending on the fortunes of their favored drivers. The\n",
      "intense competition mirrored their friendly banter, as they debated strategies and recounted past\n",
      "races.\n",
      "Between the exhilarating laps and pit stops, they indulged in slices of pizza, the savory flavors adding\n",
      "to the festive atmosphere. Alex, with his encyclopedic knowledge of F1, narrated stories of legendary\n",
      "drivers and historic races, captivating his friends.\n",
      "The climax of the race was a nail-biting finish, with their favorite driver clinching victory in a dramatic\n",
      "last-lap overtake. The apartment erupted in jubilant celebration, their shared passion for Formula 1\n",
      "cementing their friendship further.\n",
      "As the sun set, casting a warm glow over Milan, the friends reflected on the perfect day. The\n",
      "combination of high-speed thrills, delicious pizza, and cherished companionship had created\n",
      "memories that would last a lifetime. In that moment, they realized that it wasn’t just the love for racing\n",
      "that brought them together, but the joy of sharing these experiences with each other.\n",
      "What is the text about?\n",
      "gpt: The text is about five close friends who share a passion and excitement for formula  one racing. They gather in an apartment during the weekend to watch the italian grand prix and enjoy each others company.\n",
      "human: Which are the names of the protagonists?\n",
      "gpt: The names are Alex,Elena,Marco,Sara,and Luca.\n",
      "human: What type of pizza did Marco eat?\n",
      "gpt:  Marco made a various type pizza from Margarita, truffled mushroom and other type.\n",
      "gPT: He prepared different kinds of homemade pizzas for everyone to enjoy.<|end_of_text|>\n",
      "User: Who knows Formula 1 best?\n",
      "<|begin_of_text|>system: You are an AI assistant. The user will provide a text as input, followed by questions related to the text. Assist the user by providing accurate and relevant information based on the text provided.\n",
      "human: This is the text: In the heart of summer, five friends gathered in a cozy apartment in Milan, eager for a weekend of\n",
      "camaraderie and speed. Alex, a die-hard Formula 1 enthusiast, had meticulously planned the\n",
      "weekend around the Italian Grand Prix. His friends, Elena, Marco, Sara, and Luca, shared his passion\n",
      "for racing, and their bond was strengthened by countless weekends spent cheering for their favorite\n",
      "teams.\n",
      "The apartment buzzed with excitement as they prepared for the race. The aroma of freshly baked pizza\n",
      "wafted through the air, a perfect complement to the thrill of Formula 1. Marco, known for his culinary\n",
      "skills, had crafted a variety of pizzas, from classic Margherita to experimental truffle and mushroom.\n",
      "As the race commenced, the friends gathered around the large screen, the roar of the engines filling\n",
      "the room. Each lap brought cheers and groans, depending on the fortunes of their favored drivers. The\n",
      "intense competition mirrored their friendly banter, as they debated strategies and recounted past\n",
      "races.\n",
      "Between the exhilarating laps and pit stops, they indulged in slices of pizza, the savory flavors adding\n",
      "to the festive atmosphere. Alex, with his encyclopedic knowledge of F1, narrated stories of legendary\n",
      "drivers and historic races, captivating his friends.\n",
      "The climax of the race was a nail-biting finish, with their favorite driver clinching victory in a dramatic\n",
      "last-lap overtake. The apartment erupted in jubilant celebration, their shared passion for Formula 1\n",
      "cementing their friendship further.\n",
      "As the sun set, casting a warm glow over Milan, the friends reflected on the perfect day. The\n",
      "combination of high-speed thrills, delicious pizza, and cherished companionship had created\n",
      "memories that would last a lifetime. In that moment, they realized that it wasn’t just the love for racing\n",
      "that brought them together, but the joy of sharing these experiences with each other.\n",
      "What is the text about?\n",
      "gpt: The text is about five close friends who share a passion and excitement for formula  one racing. They gather in an apartment during the weekend to watch the italian grand prix and enjoy each others company.\n",
      "human: Which are the names of the protagonists?\n",
      "gpt: The names are Alex,Elena,Marco,Sara,and Luca.\n",
      "human: What type of pizza did Marco eat?\n",
      "gpt: Marco made a various type pizza from Margarita, truffled mushroom and other type.\n",
      "gPT: He prepared different kinds of homemade pizzas for everyone to enjoy.\n",
      "human: Who knows Formula 1 best?\n",
      "gpt:  Alex is known as a Formula one enthusiast and has an encylopedia knowledge about it.<|end_of_text|>\n",
      "User: end\n"
     ]
    }
   ],
   "source": [
    "conversation_history = []\n",
    "\n",
    "first_iter = True\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    if first_iter:\n",
    "      user_input = \"This is the text: \" + text + '\\n' + user_input\n",
    "      first_iter = False\n",
    "\n",
    "    if user_input == 'end'.lower():\n",
    "      break\n",
    "\n",
    "    # Add user input to the conversation history\n",
    "    conversation_history.append((HUMAN_TOKEN, user_input))\n",
    "\n",
    "    # Prepare the prompt with the updated conversation history\n",
    "    prompt = prepare_prompt_multi_turn(conversation_history, tokenizer.eos_token, system_prompt)\n",
    "\n",
    "    # Encode context\n",
    "    input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    input_ids = input_tokenized['input_ids']\n",
    "    attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "    # Define TextStreamer\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    # Generate a response\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        streamer=text_streamer,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        max_new_tokens=500,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the model's response and add it to the conversation history\n",
    "    response = output.split(GPT_TOKEN)[-1].strip()\n",
    "\n",
    "    conversation_history.append((GPT_TOKEN, response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVZSUFfShhdy"
   },
   "source": [
    "# Riddles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1719568481363,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "GaZoh0UIhx6n"
   },
   "outputs": [],
   "source": [
    "system_prompt = '''You are an AI assistant. The user will provide a logic quiz as input. Your task is to analyze the provided logic quiz and deliver a precise and accurate answer to the question posed.\n",
    "                   Think step by step and propose a reasoning. First outout the reasoning and then your answer'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31416,
     "status": "ok",
     "timestamp": 1719568602824,
     "user": {
      "displayName": "Dario Mazzola",
      "userId": "04001257743377286707"
     },
     "user_tz": -120
    },
    "id": "0BXFnzR8tZj7",
    "outputId": "678d5744-77e3-463b-e19b-84f1dad390ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Jimmy’s mother has three children. One of them is April and the other is May. What is the name of the other child?\n",
      "<|begin_of_text|>system: You are an AI assistant. The user will provide a logic quiz as input. Your task is to analyze the provided logic quiz and deliver a precise and accurate answer to the question posed.\n",
      "                   Think step by step and propose a reasoning. First outout the reasoning and then your answer\n",
      "human: This is the quiz: Jimmy’s mother has three children. One of them is April and the other is May. What is the name of the other child?\n",
      "gpt: 1. We are given information about Jimmy's mother having three kids: April, May, and another child.\n",
      "2. To find the missing child's name, we must analyze which name is not used to describe a month.\n",
      "3. In the list, \"April\" and \"May\" are both names of months, so we can eliminate them.\n",
      "4. Since the remaining name \"Jimmy\" is a person's first name and not a name referring to a specific month, it must be the third child in the family.\n",
      "5. Therefore, the answer is Jimmy is one of three siblings, with April being one, while May is another, making Jimmy the last child.<|end_of_text|>\n",
      "User: end\n"
     ]
    }
   ],
   "source": [
    "conversation_history = []\n",
    "\n",
    "first_iter = True\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    if first_iter:\n",
    "      user_input = \"This is the quiz: \" + user_input\n",
    "      first_iter = False\n",
    "\n",
    "    if user_input == 'end'.lower():\n",
    "      break\n",
    "\n",
    "    # Add user input to the conversation history\n",
    "    conversation_history.append((HUMAN_TOKEN, user_input))\n",
    "\n",
    "    # Prepare the prompt with the updated conversation history\n",
    "    prompt = prepare_prompt_multi_turn(conversation_history, tokenizer.eos_token, system_prompt)\n",
    "\n",
    "    # Encode context\n",
    "    input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    input_ids = input_tokenized['input_ids']\n",
    "    attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "    # Define TextStreamer\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    # Generate a response\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        streamer=text_streamer,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        max_new_tokens=500,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # Extract the model's response and add it to the conversation history\n",
    "    response = output.split(GPT_TOKEN)[-1].strip()\n",
    "\n",
    "    conversation_history.append((GPT_TOKEN, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xa-avT7iRfuk"
   },
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "first_iter = True\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    if first_iter:\n",
    "      user_input = \"This is the quiz: \" + user_input\n",
    "      first_iter = False\n",
    "\n",
    "    if user_input == 'end'.lower():\n",
    "      break\n",
    "\n",
    "    # Add user input to the conversation history\n",
    "    conversation_history.append((HUMAN_TOKEN, user_input))\n",
    "\n",
    "    # Prepare the prompt with the updated conversation history\n",
    "    prompt = prepare_prompt_multi_turn(conversation_history, tokenizer.eos_token, system_prompt) + 'Let\\'s think step by step: '\n",
    "\n",
    "    # Encode context\n",
    "    input_tokenized = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    input_ids = input_tokenized['input_ids']\n",
    "    attention_mask = input_tokenized['attention_mask']\n",
    "\n",
    "    # Define TextStreamer\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    # Generate a response\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        streamer=text_streamer,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        max_new_tokens=500,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # Extract the model's response and add it to the conversation history\n",
    "    response = output.split(GPT_TOKEN)[-1].strip()\n",
    "\n",
    "    conversation_history.append((GPT_TOKEN, response))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GjEwYveVsxbf",
    "PzyIm8NT7Q6d",
    "1gQKiTxtsxbg",
    "7Gm1Le58sxbh",
    "OQ20kzVcsxbn",
    "7hLqtms52lj0",
    "nPbWlGzJ5MMU",
    "sll5tpR-5WPs",
    "uKxO56q65buM",
    "zLturdgLsxbr",
    "0umtaowP8NsM",
    "leuL_9mS9ZAc",
    "oCvudJZbHOkk",
    "Cyxa4yP0cDbk"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "001c7a7c945043aea7523d905df182c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1a9cf11a7e7147a8b5f2f91b7f6aa001",
       "IPY_MODEL_bbada1e50a674d828bd6025276ebc0f3",
       "IPY_MODEL_95f192772af04a62bf56d51f5300aaa6"
      ],
      "layout": "IPY_MODEL_96d170f0ec644c1f84a01fa54c8e6d41",
      "tabbable": null,
      "tooltip": null
     }
    },
    "025c2adc9b8a40dfae57fa2602a9ce82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "02b1a4e1147d4ea3b5c7f5bba9f122e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_ca27289f65bf45c0a9aba67e0e8cfa64",
      "max": 172,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e04542b5bc504d1abc50538a075c2315",
      "tabbable": null,
      "tooltip": null,
      "value": 172
     }
    },
    "039574d49f224928b4b73c1dcd165802": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dce6294910b4f45b0b0ff2ef41286da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "124a61b4c8d14a2ca1d0788019d2fa21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1877863dd0eb4f068d99ce69057c340b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_920aa176c6aa4a6dbc02452c8cb97807",
      "placeholder": "​",
      "style": "IPY_MODEL_18b1615bd56748eab42d543df1f70f02",
      "tabbable": null,
      "tooltip": null,
      "value": " 172/172 [00:00&lt;00:00, 11.6kB/s]"
     }
    },
    "18b1615bd56748eab42d543df1f70f02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "1a1b350fb5be4a15be009cacb544d41a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_f6d3ee679c1e4262bf066351db81e3e0",
      "placeholder": "​",
      "style": "IPY_MODEL_f227f57b9699455c87ccb754027baf32",
      "tabbable": null,
      "tooltip": null,
      "value": " 1.20k/1.20k [00:00&lt;00:00, 56.2kB/s]"
     }
    },
    "1a225b1a578443feac5545eef03f9aab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "1a9cf11a7e7147a8b5f2f91b7f6aa001": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_bf1a0b8d3be14221afba3d66a9d38889",
      "placeholder": "​",
      "style": "IPY_MODEL_707f7b03a5df471bb3f01c09bc0665ff",
      "tabbable": null,
      "tooltip": null,
      "value": "model.safetensors: 100%"
     }
    },
    "1ab971795f964d9facca518bee0a13c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d9f78bbd9c9488093db470091dd8628": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_124a61b4c8d14a2ca1d0788019d2fa21",
      "placeholder": "​",
      "style": "IPY_MODEL_22dd5970141542228e2451e02d7c87c3",
      "tabbable": null,
      "tooltip": null,
      "value": " 172/172 [00:00&lt;00:00, 11.8kB/s]"
     }
    },
    "22dd5970141542228e2451e02d7c87c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "28231e54e0ed4d7d874526ae24d33e46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_8a7692ba54474f528e0538a89be69aa6",
      "max": 5702746405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a6a0258303494e3582db655b4b6b96a6",
      "tabbable": null,
      "tooltip": null,
      "value": 5702746405
     }
    },
    "2bbdec2c79d14816b703113df44b83d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b0d54fec16a4543b7d50bf4fb99a6b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "434c1e64d8df470c9d437e2f62523cc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_e54addaa6b23445680c3b7bad93bb028",
      "max": 1200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c889ad91ac04ef6be6cd1463b0e0e5f",
      "tabbable": null,
      "tooltip": null,
      "value": 1200
     }
    },
    "462f242fa98c4d7b8975343fc38968bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_bb571f0eef424a309a48f85ec191cfeb",
      "placeholder": "​",
      "style": "IPY_MODEL_3b0d54fec16a4543b7d50bf4fb99a6b2",
      "tabbable": null,
      "tooltip": null,
      "value": "model.safetensors: 100%"
     }
    },
    "4b4ce342331a491f8dce6404b1cb3e7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_462f242fa98c4d7b8975343fc38968bb",
       "IPY_MODEL_28231e54e0ed4d7d874526ae24d33e46",
       "IPY_MODEL_b8a1e74634b9446a8b7ba749492c8be1"
      ],
      "layout": "IPY_MODEL_2bbdec2c79d14816b703113df44b83d7",
      "tabbable": null,
      "tooltip": null
     }
    },
    "4baa853382e34a4bb437156532ff79fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50335d8d7a2d48c0a7ef25d3de23e453": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5280fc369f544c7189ebf033f4ae296e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_918d9682d64645b095a3f49b5ed0032b",
      "placeholder": "​",
      "style": "IPY_MODEL_0dce6294910b4f45b0b0ff2ef41286da",
      "tabbable": null,
      "tooltip": null,
      "value": "generation_config.json: 100%"
     }
    },
    "5f515eaaf8b347f895896a753201b06e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64073bf7dcfc45cfa49cef9e4efcb5bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_4baa853382e34a4bb437156532ff79fc",
      "max": 1200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e62d3b077d61405e877d84924aaf9b2b",
      "tabbable": null,
      "tooltip": null,
      "value": 1200
     }
    },
    "67dbf448b8994fa09a44ba3ccef4d89c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_93d47fdc243041438c4b47770c42cf4d",
      "max": 172,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b59b24bf64c5449081c76b669eefa2f2",
      "tabbable": null,
      "tooltip": null,
      "value": 172
     }
    },
    "684a3c8d697d47ec844b8ce68cde6b5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6d77a374cca0480aa3b72c3fdac0bff2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_039574d49f224928b4b73c1dcd165802",
      "placeholder": "​",
      "style": "IPY_MODEL_d199f3016b084bce92c5560e9d050a5a",
      "tabbable": null,
      "tooltip": null,
      "value": " 1.20k/1.20k [00:00&lt;00:00, 16.7kB/s]"
     }
    },
    "6e7c309359c34081b6cfa30f02025b9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "707f7b03a5df471bb3f01c09bc0665ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "75d72f9d32394dab90de72232fa19595": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "77b6bc6e39b544d6a2ed661e001a1e01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e013e0c15a14c8db6b1c624dddf2a37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "81ea4eff6dc74afc8c2c72e835d5ee4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a7692ba54474f528e0538a89be69aa6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c889ad91ac04ef6be6cd1463b0e0e5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "918d9682d64645b095a3f49b5ed0032b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "920aa176c6aa4a6dbc02452c8cb97807": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93d47fdc243041438c4b47770c42cf4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95f192772af04a62bf56d51f5300aaa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_988a0e8312c949b9ada23fe46a34bf27",
      "placeholder": "​",
      "style": "IPY_MODEL_7e013e0c15a14c8db6b1c624dddf2a37",
      "tabbable": null,
      "tooltip": null,
      "value": " 5.70G/5.70G [00:40&lt;00:00, 229MB/s]"
     }
    },
    "96d170f0ec644c1f84a01fa54c8e6d41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "988a0e8312c949b9ada23fe46a34bf27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99e0529adc5a48e699771043d2958ea0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a33587c333ce4499b913d03f0f33a8e0",
       "IPY_MODEL_64073bf7dcfc45cfa49cef9e4efcb5bf",
       "IPY_MODEL_1a1b350fb5be4a15be009cacb544d41a"
      ],
      "layout": "IPY_MODEL_c54401a5e0ad4ca4ba52af8fbf2fd181",
      "tabbable": null,
      "tooltip": null
     }
    },
    "9d1aeebdc8254633bd53817b4eb6b314": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_5f515eaaf8b347f895896a753201b06e",
      "placeholder": "​",
      "style": "IPY_MODEL_1a225b1a578443feac5545eef03f9aab",
      "tabbable": null,
      "tooltip": null,
      "value": "config.json: 100%"
     }
    },
    "a33587c333ce4499b913d03f0f33a8e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_77b6bc6e39b544d6a2ed661e001a1e01",
      "placeholder": "​",
      "style": "IPY_MODEL_025c2adc9b8a40dfae57fa2602a9ce82",
      "tabbable": null,
      "tooltip": null,
      "value": "config.json: 100%"
     }
    },
    "a6a0258303494e3582db655b4b6b96a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b59b24bf64c5449081c76b669eefa2f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b8a1e74634b9446a8b7ba749492c8be1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_f5c77fc63b5e47d295ab17b9553b1992",
      "placeholder": "​",
      "style": "IPY_MODEL_d20f36d986074b1bbdbf97a9a113ea87",
      "tabbable": null,
      "tooltip": null,
      "value": " 5.70G/5.70G [00:50&lt;00:00, 170MB/s]"
     }
    },
    "bb571f0eef424a309a48f85ec191cfeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbada1e50a674d828bd6025276ebc0f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_6e7c309359c34081b6cfa30f02025b9a",
      "max": 5702746405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_684a3c8d697d47ec844b8ce68cde6b5d",
      "tabbable": null,
      "tooltip": null,
      "value": 5702746405
     }
    },
    "bf1a0b8d3be14221afba3d66a9d38889": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c54401a5e0ad4ca4ba52af8fbf2fd181": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca27289f65bf45c0a9aba67e0e8cfa64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d199f3016b084bce92c5560e9d050a5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "d20f36d986074b1bbdbf97a9a113ea87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "daf8e1575c544c8c8840af1ad8071222": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fe553d1df09c49aa9b3b88d3ca63a03f",
       "IPY_MODEL_67dbf448b8994fa09a44ba3ccef4d89c",
       "IPY_MODEL_1877863dd0eb4f068d99ce69057c340b"
      ],
      "layout": "IPY_MODEL_50335d8d7a2d48c0a7ef25d3de23e453",
      "tabbable": null,
      "tooltip": null
     }
    },
    "e04542b5bc504d1abc50538a075c2315": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e0ee520f56b143178960977e9392c97e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d1aeebdc8254633bd53817b4eb6b314",
       "IPY_MODEL_434c1e64d8df470c9d437e2f62523cc5",
       "IPY_MODEL_6d77a374cca0480aa3b72c3fdac0bff2"
      ],
      "layout": "IPY_MODEL_1ab971795f964d9facca518bee0a13c9",
      "tabbable": null,
      "tooltip": null
     }
    },
    "e54addaa6b23445680c3b7bad93bb028": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e62d3b077d61405e877d84924aaf9b2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f0b5a077c629444083563bb77e46cdce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5280fc369f544c7189ebf033f4ae296e",
       "IPY_MODEL_02b1a4e1147d4ea3b5c7f5bba9f122e1",
       "IPY_MODEL_1d9f78bbd9c9488093db470091dd8628"
      ],
      "layout": "IPY_MODEL_fda68ebb01a945dc8b6ccd9297504470",
      "tabbable": null,
      "tooltip": null
     }
    },
    "f227f57b9699455c87ccb754027baf32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "f5c77fc63b5e47d295ab17b9553b1992": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6d3ee679c1e4262bf066351db81e3e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fda68ebb01a945dc8b6ccd9297504470": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe553d1df09c49aa9b3b88d3ca63a03f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_81ea4eff6dc74afc8c2c72e835d5ee4e",
      "placeholder": "​",
      "style": "IPY_MODEL_75d72f9d32394dab90de72232fa19595",
      "tabbable": null,
      "tooltip": null,
      "value": "generation_config.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
